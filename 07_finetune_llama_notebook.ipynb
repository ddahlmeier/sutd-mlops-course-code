{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b44ac5-a6e5-4deb-bf47-9daf2db61b1a",
   "metadata": {},
   "source": [
    "# Finetune OpenLlama in Sagemaker notebook \n",
    "\n",
    "In this notebook, we will perform instruction tuning OpenLlama using a subset of the Dolly 15k Dataset.\n",
    "\n",
    "\n",
    "This notebook as been put together based a few great examples and blogs. Feel free to visit them to learn more about finetuning. \n",
    "\n",
    "- [Fourthbrain Repository Building with Instruction-Tuned LLMs: a Step-by-Step Guide](https://github.com/FourthBrain/Building-with-Instruction-Tuned-LLMs-A-Step-by-Step-Guide)\n",
    "- [Notes on fine-tuning Llama 2 using QLoRA: A detailed breakdown. Blog by Ogban Ugot](https://medium.com/@ogbanugot/notes-on-fine-tuning-llama-2-using-qlora-a-detailed-breakdown-370be42ccca1)\n",
    "- [Interactively fine-tune Falcon-40B and other LLMs on Amazon SageMaker Studio notebooks using QLoRA. Blog by AWS](https://aws.amazon.com/blogs/machine-learning/interactively-fine-tune-falcon-40b-and-other-llms-on-amazon-sagemaker-studio-notebooks-using-qlora/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12aa73-eed1-4b34-a578-5bf0fb8b4ec5",
   "metadata": {},
   "source": [
    "### ⚠ IMPORTANT ⚠\n",
    "\n",
    "Please ensure your Jupyterlab instance is set to the following: **ml.g5.4xlarge**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a983a-99df-4cbd-b8fc-cd51a4d48a93",
   "metadata": {},
   "source": [
    "# Development environment\n",
    "\n",
    "We're going to be leveraging a number of awesome tools in order to be able to instruct-tune our model.\n",
    "\n",
    "Here's a brief overview:\n",
    "\n",
    "- [Hugging Face's PEFT Library](https://github.com/huggingface/peft)\n",
    "- [Hugging Face's Transformers Library](https://huggingface.co/docs/transformers/index)\n",
    "- [QLoRA](https://arxiv.org/abs/2305.14314)\n",
    "- [TRL](https://github.com/lvwerra/trl/tree/main/docs/source)\n",
    "\n",
    "Keep in mind that these libraries are being constantly iterated on - and so you may experience bugs/issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985305a-85aa-48de-a454-9fe506fc8d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install peft==0.4.0 \n",
    "! pip install bitsandbytes==0.40.2 \n",
    "! pip install transformers==4.31.0 \n",
    "! pip install trl==0.4.7\n",
    "! pip install torch==2.0.1\n",
    "! pip install accelerate==0.21.0\n",
    "! pip install datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5caf4-a7d9-41d7-9b3e-4131ce0d9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add installed cuda runtime to path for bitsandbytes\n",
    "import os\n",
    "import nvidia\n",
    "\n",
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75ef5b-a947-43bb-8a75-718db642de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7f305-bcd6-4ef9-8efc-e6b4eac00162",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5361b-9cdc-4898-923b-7d22c8f499d1",
   "metadata": {},
   "source": [
    "Let's look at our dataset to get an idea of what we're working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba2329-b6ae-43d5-aa50-f4b9f7d969d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dbricks_15k_dataset_base = load_dataset(\"databricks/databricks-dolly-15k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621b461-2c02-4194-b28f-9efc63ac4a0c",
   "metadata": {},
   "source": [
    "Let's check out some brief stats about our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578c974-939d-4cac-8637-484ccb4dbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "def plot_and_filter_sequence_lengths(dataset_obj, max_length=2200):\n",
    "\n",
    "    # Initialize a list to store the sequence lengths\n",
    "    sequence_lengths = []\n",
    "\n",
    "    # list of indices that are too long\n",
    "    too_long = []\n",
    "\n",
    "    # Loop over the dataset and get the lengths of text sequences\n",
    "    for idx, example in enumerate(dataset_obj[\"train\"]):\n",
    "        sequence_lengths.append(len(example['instruction']) + len(example[\"context\"]) + len(example[\"response\"]))\n",
    "        if sequence_lengths[idx] > max_length:\n",
    "          too_long.append(idx)\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.hist(sequence_lengths, bins=30)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Text Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "    return too_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f29c1-9489-41e4-aacd-89b8c0bff1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_drop = plot_and_filter_sequence_lengths(dbricks_15k_dataset_base, max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54dd1b-337a-4887-ad85-be2ab5c4f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indexes_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960bf20-6b6e-4b2b-91be-bab18562d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbricks_15k_dataset_reduced = dbricks_15k_dataset_base[\"train\"].select(\n",
    "    i for i in range(len(dbricks_15k_dataset_base[\"train\"])) if i not in set(indexes_to_drop)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11cd891-055e-4b4d-85c2-dabe16639d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbricks_15k_dataset_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569c807-3667-442e-9117-100df77e458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbricks_15k_dataset_prepared = dbricks_15k_dataset_reduced.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454534c-e1d6-4896-bd91-7ec754c7c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_filter_sequence_lengths(dbricks_15k_dataset_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527d820-82f0-4a0a-ab3b-ec6a5dca9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbricks_15k_dataset_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef5d6b-e8c7-4d9c-aaa8-bf8b6898ac00",
   "metadata": {},
   "source": [
    "Before we can begin training, we need to set up a few helper functions to ensure our dataset is parsed in the correct format and we save our PEFT adapters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6118dbe-0689-4e20-9ce3-ba2748769872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "  if example.get(\"context\", \"\") != \"\":\n",
    "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "      \"### Instruction:\\n\"\n",
    "      f\"{example['instruction']}\\n\\n\"\n",
    "      f\"### Input: \\n\"\n",
    "      f\"{example['context']}\\n\\n\"\n",
    "      f\"### Response: \\n\"\n",
    "      f\"{example['response']}\")\n",
    "\n",
    "  else:\n",
    "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
    "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "      \"### Instruction:\\n\"\n",
    "      f\"{example['instruction']}\\n\\n\"\n",
    "      f\"### Response:\\n\"\n",
    "      f\"{example['response']}\")\n",
    "\n",
    "  return {\"text\" : input_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1cf4a-7be1-40f9-8657-14869b99df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = dbricks_15k_dataset_prepared.map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f62744-73d1-4d6b-9c45-633cc5a47364",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78417b-b196-4a93-8748-2aaeb55453e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(formatted_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4307c-06a1-43b1-9e71-e6c3b4517981",
   "metadata": {},
   "source": [
    "Okay, now that we have the Dolly 15k dataset pared down to a more reasonable length - let's set up our model!\n",
    "\n",
    "We'll be leveraging QLoRA for this portion of the notebook, which will ensure a low memory footprint during fine-tuning!\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2305.14314.pdf)\n",
    "- [Blog](https://huggingface.co/blog/4bit-transformers-bitsandbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a3f56-ed2f-4d5b-b45c-4cf0afeb56c1",
   "metadata": {},
   "source": [
    "# Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9ea2e-f7b9-4cf8-bee9-c72aa69645e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"openlm-research/open_llama_3b_v2\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"open_llama_3b_v2_chat_dolly\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe6f16-3330-4356-9566-eb6767762675",
   "metadata": {},
   "source": [
    "Now, let's set up our SupervisedFineTuningTrainer and let it rip!\n",
    "\n",
    "More information on the SFTTrainer is available here:\n",
    "\n",
    "- [HF Documentation](https://huggingface.co/docs/trl/main/en/sft_trainer)\n",
    "- [Repository](https://github.com/lvwerra/trl/blob/main/trl/trainer/sft_trainer.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fd3e5-e3d7-447e-8d2c-d45efed2c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835276e-ac1e-4ec0-8bf1-d711295e4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2413bf59-3ea8-4d1d-af59-7f9ce5d3fdcd",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e719598-be19-4f18-ac76-ec56e670a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate and return the metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055073b-e296-4014-a8a1-3cdada8961aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708347ea-040d-4327-9a96-87194109c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc7001-21a1-41c0-b14a-5872e97d4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c4ba7-c178-4cb4-9548-4e94fbc95e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(new_model, use_temp_dir=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff551e6-efdc-479f-9df0-32453f153db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac252c-bbe3-432f-a543-00737b33e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "lora_config = LoraConfig.from_pretrained(new_model)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    lora_config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83847d-80fa-43f4-8f9f-dba4b6517429",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1289d-d962-478a-8d77-a6fe083c521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def make_inference(instruction, context = None):\n",
    "  if context:\n",
    "    prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "  else:\n",
    "    prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda:0\")\n",
    "  outputs = base_model.generate(**inputs, max_new_tokens=100)\n",
    "  print(\"### Basemodel\")\n",
    "  display(Markdown((tokenizer.decode(outputs[0], skip_special_tokens=True))))\n",
    "  outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "  print(\"### Finetuned model\")\n",
    "  display(Markdown((tokenizer.decode(outputs[0], skip_special_tokens=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beee242-b4d6-4d9d-8ef3-9a94c8f3e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_inference(\"Explain the moon landing to a 5 year old.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda4be7-4a60-4be6-b199-b2ae4b94fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_inference(\"Identify the odd one out and explain your choice.\", \"Orange, Green, Airplane.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56a93c-aca6-43a9-ac08-d34097c129bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_inference(\"When was Kyoto the capital of Japan?\", \"Kyoto is one of the oldest municipalities in Japan, having been chosen in 794 as the new seat of Japan's imperial court by Emperor Kanmu. The original city, named Heian-kyō, was arranged in accordance with traditional Chinese feng shui following the model of the ancient Chinese capitals of Chang'an and Luoyang. The emperors of Japan ruled from Kyoto in the following eleven centuries until 1869. It was the scene of several key events of the Muromachi period, Sengoku period, and the Boshin War, such as the Ōnin War, the Honnō-ji Incident, the Kinmon incident and the Battle of Toba–Fushimi. The capital was relocated from Kyoto to Tokyo after the Meiji Restoration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f342942-3c11-4463-8b15-65600be4eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_inference(\"Where is Hoober Stand located?\", \"Hoober Stand is a 30-metre-high (98 ft) tower and Grade II* listed building on a ridge in Wentworth, South Yorkshire in northern England. It was designed by Henry Flitcroft for the Whig aristocrat Thomas Watson-Wentworth, Earl of Malton (later the 1st Marquess of Rockingham) to commemorate the quashing of the 1745 Jacobite rebellion. It lies close to his country seat Wentworth Woodhouse. Its site is approximately 157 metres (515 ft) above sea level and from the top there are long-distance views on a clear day. Hoober Stand is one of several follies in and around Wentworth Woodhouse park; the others include Needle's Eye and Keppel's Column. Sidney Oldall Addy, the Sheffield author calls the structure Woburn Stand in his 1888 book, A glossary of words used in the neighbourhood of Sheffield.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56726dc3-6cc0-4e2c-b492-d5636d6d8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_inference(\"Given this paragraph about hedgehogs, why are they different from porcupines?\", \"Hedgehogs are easily recognized by their spines, which are hollow hairs made stiff with keratin.Their spines are not poisonous or barbed and, unlike the quills of a porcupine, do not easily detach from their bodies. However, the immature animal's spines normally fall out as they are replaced with adult spines. This is called \\\"quilling\\\". Spines can also shed when the animal is diseased or under extreme stress. Hedgehogs are usually brown, with pale tips to the spines, though blonde hedgehogs are found on the Channel Island of Alderney.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8618c53-a1f2-4d11-b31a-c8e0b9c202d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_inference(\"Given this paragraph, who wrote and directed Heads I Win, Tails You Lose\", \"Heads I Win, Tails You Lose (Italian: Testa o Croce, also known as Heads or Tails) is a 1982 Italian comedy film written and directed by Nanni Loy.\\n\\nThe film consists in two back-to-back stories that deals with two \\\"taboo\\\" themes, the celibacy of the clergy in the episode of Renato Pozzetto and the homosexuality in the one with Nino Manfredi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b38b5e-3d90-4300-b9d0-50ba64d9a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_inference(\"Why is free climbing called free climbing\", \"Most of the climbing done in modern times is considered free climbing—climbing using one's own physical strength, with equipment used solely as protection and not as support—as opposed to aid climbing, the gear-dependent form of climbing that was dominant in the sport's earlier days. Free climbing is typically divided into several styles that differ from one another depending on the choice of equipment used and the configurations of their belay, rope and anchor systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bced63-55d0-4e8e-81dc-c53c9f01ad3e",
   "metadata": {},
   "source": [
    "# What to do next\n",
    "- Understand the finetuning parameters\n",
    "- Is there any difference to the untrained model?\n",
    "- How well is ChatGPT doing on this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609257a0-c62b-4298-923f-767b0d25ac97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
