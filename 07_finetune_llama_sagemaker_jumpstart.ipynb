{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Fine-tune Llama 2 models on SageMaker JumpStart\n",
    "In this notebook, we will perform instruction tuning Llama 2 using a subset of the Dolly 15k Dataset using Sagemaker Jumpstart.\n",
    "\n",
    "This notebook as been put together based on an example from the Generative AI on AWS book https://github.com/generative-ai-on-aws/generative-ai-on-aws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9539dd-ad5e-4fd9-bc65-b75dc0ea4db0",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b674854d-4a29-4737-afe0-d8b9b530e71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets[s3] in /opt/conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (1.26.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets[s3]) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (0.19.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (6.0.1)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (2023.6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets[s3]) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[s3]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[s3]) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[s3]) (2023.3)\n",
      "Requirement already satisfied: aiobotocore~=2.5.0 in /opt/conda/lib/python3.10/site-packages (from s3fs->datasets[s3]) (2.5.4)\n",
      "Requirement already satisfied: botocore<1.31.18,>=1.31.17 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->datasets[s3]) (1.31.17)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->datasets[s3]) (1.16.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->datasets[s3]) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets[s3]) (1.16.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs->datasets[s3]) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets[s3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "---\n",
    "\n",
    "First we will deploy the Llama-2 7B model as a SageMaker endpoint and do some inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e1d98f",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"2.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For forward compatibility, pin to model_version='2.*' in your JumpStartModel or JumpStartEstimator definitions. Note that major version upgrades may have different EULA acceptance terms and input/output signatures.\n",
      "Using model 'meta-textgeneration-llama-2-7b' with wildcard version identifier '2.*'. You can pin to version '2.1.8' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "For forward compatibility, pin to model_version='2.*' in your JumpStartModel or JumpStartEstimator definitions. Note that major version upgrades may have different EULA acceptance terms and input/output signatures.\n",
      "Using model 'meta-textgeneration-llama-2-7b' with wildcard version identifier '2.*'. You can pin to version '2.1.8' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd833f8-1ddc-4805-80b2-19e7db629880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to be happy.\n",
      "I think the meaning of life is to be happy.\n",
      "I think the meaning of life is to be happy and make other people happy.\n",
      "I think the meaning of life is to be happy and to make others happy.\n",
      "I think the meaning of life is to be happy and make others\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = pretrained_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e16f-d459-40c6-9d6b-0272938b3878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. We will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load training data set\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# Filter for question answering examples\n",
    "qa_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"closed_qa\")\n",
    "qa_dataset = qa_dataset.remove_columns(\"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3b29698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_and_filter_sequence_lengths(dataset_obj, max_length=2200):\n",
    "\n",
    "    # Initialize a list to store the sequence lengths\n",
    "    sequence_lengths = []\n",
    "\n",
    "    # list of indices that are too long\n",
    "    too_long = []\n",
    "\n",
    "    # Loop over the dataset and get the lengths of text sequences\n",
    "    for idx, example in enumerate(dataset_obj):\n",
    "        sequence_lengths.append(len(example['instruction']) + len(example[\"context\"]) + len(example[\"response\"]))\n",
    "        if sequence_lengths[idx] > max_length:\n",
    "          too_long.append(idx)\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.hist(sequence_lengths, bins=30)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Text Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "    return too_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b17820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter long tail of very long instances\n",
    "indexes_to_drop = plot_and_filter_sequence_lengths(qa_dataset, max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c65dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object <genexpr> at 0x7fb994184510> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "qa_dataset_reduced = qa_dataset.select(\n",
    "    i for i in range(len(qa_dataset)) if i not in set(indexes_to_drop)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d5fb87-b991-4463-ab0e-00ca1336c319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response'],\n",
       "    num_rows: 935\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b49f9e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df215e3e27f4b45b2f11320afd5a45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'context': 'Kilo is a station on the VR commuter rail network on the '\n",
      "               'Rantarata line located in Kilo, a district of the city of '\n",
      "               'Espoo in Finland. It is situated between Leppävaara railway '\n",
      "               'station and Kera railway station, approximately 13 kilometres '\n",
      "               '(8.1 mi) northwest/west of Helsinki Central railway station.',\n",
      "    'instruction': 'Where is the Kilo railway station',\n",
      "    'response': 'Kilo is a station on the VR commuter rail network on the '\n",
      "                'Rantarata line located in Kilo, a district of the city of '\n",
      "                'Espoo in Finland. It is situated between Leppavaara railway '\n",
      "                'station and Kera railway station, approximately 13 kilometres '\n",
      "                '(8.1 mi) northwest/west of Helsinki Central railway station.'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = qa_dataset_reduced.train_test_split(test_size=0.1)\n",
    "train_dataset = train_and_test_dataset[\"train\"]\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_dataset.to_json(\"training.jsonl\")\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we create a prompt template for using the data in an instruction / input format for the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \"{response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92ac02d1-3a59-4779-8a43-42262302bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Training data: s3://sagemaker-us-east-1-703877312554/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"training.jsonl\"\n",
    "train_data_location = f\"s3://{bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the model\n",
    "\n",
    "Next, we fine-tune the LLaMA v2 7B model. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2024-01-20-10-11-13-459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-20 10:11:13 Starting - Starting the training job...\n",
      "2024-01-20 10:11:21 Pending - Training job waiting for capacity...\n",
      "2024-01-20 10:11:57 Pending - Preparing the instances for training....................................\n",
      "2024-01-20 10:18:15 Downloading - Downloading input data................................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-20 10:23:23,524 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-20 10:23:23,584 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-20 10:23:23,593 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-20 10:23:23,595 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-20 10:23:31,460 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=38739dd74085c7be3fb58808f8417b7ebdb2603d2dff8cbc4d2840ebf5cc12ab\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\n",
      "2024-01-20 10:23:22 Training - Training image download completed. Training in progress.\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-01-20 10:24:29,076 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-20 10:24:29,076 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-20 10:24:29,138 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-20 10:24:29,209 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-20 10:24:29,279 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-20 10:24:29,288 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"1\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2024-01-20-10-11-13-459\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"1\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"1\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2024-01-20-10-11-13-459\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"1\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=1\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 1 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-01-20 10:24:29,315 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '1', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2024-01-20 10:24:34,591] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-01-20 10:24:34,591] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-01-20 10:24:34,591] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-01-20 10:24:34,591] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1301.77it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mGenerating train split: 1910 examples [00:00, 84025.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 1901/1910 [00:00<00:00, 18898.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 18459.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 18773.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 18602.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 18778.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 18605.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 18948.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 18774.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 1000/1910 [00:02<00:02, 388.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 1000/1910 [00:02<00:02, 386.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 1000/1910 [00:02<00:02, 388.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 1000/1910 [00:02<00:02, 387.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:03<00:00, 565.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:03<00:00, 563.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:03<00:00, 527.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:03<00:00, 565.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:03<00:00, 524.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:03<00:00, 526.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:03<00:00, 564.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:03<00:00, 526.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1910 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 1000/1910 [00:00<00:00, 1782.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 1000/1910 [00:00<00:00, 1711.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 1000/1910 [00:00<00:00, 1701.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 1000/1910 [00:00<00:00, 1648.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 2561.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 2387.72 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 2498.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 2321.00 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 2454.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 2286.01 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 2403.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1910/1910 [00:00<00:00, 2233.46 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.82s/it]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.43s/it]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.81s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.93s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.24s/it]\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 598\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 150\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/37 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/37 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda11.8\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/37 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/37 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/37 [00:10<06:18, 10.52s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.7051764726638794\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/37 [00:10<06:23, 10.66s/it]#015Training Epoch0:   3%|#033[34m▎         #033[0m| 1/37 [00:10<06:24, 10.68s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/37 [00:10<06:20, 10.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 2/37 [00:20<05:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.5168089866638184\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 2/37 [00:20<05:56, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 2/37 [00:20<05:55, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 2/37 [00:20<05:57, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 3/37 [00:30<05:40, 10.03s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 3/37 [00:30<05:41, 10.05s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 3/37 [00:30<05:40, 10.02s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.4438461065292358\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 3/37 [00:30<05:41, 10.05s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.4194762706756592\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 4/37 [00:40<05:29,  9.98s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 4/37 [00:40<05:28,  9.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 4/37 [00:40<05:29,  9.98s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 4/37 [00:40<05:28,  9.96s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.6148133277893066\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 5/37 [00:50<05:18,  9.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 5/37 [00:50<05:18,  9.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 5/37 [00:50<05:18,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 5/37 [00:50<05:17,  9.94s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.3197698593139648\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 6/37 [01:00<05:07,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 6/37 [01:00<05:07,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 6/37 [00:59<05:07,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 6/37 [00:59<05:07,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 7/37 [01:09<04:57,  9.91s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.2886472940444946\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 7/37 [01:09<04:57,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 7/37 [01:09<04:57,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 7/37 [01:09<04:57,  9.91s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.318097472190857\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 8/37 [01:19<04:47,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 8/37 [01:19<04:47,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 8/37 [01:19<04:46,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 8/37 [01:19<04:47,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 9/37 [01:29<04:36,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 9/37 [01:29<04:36,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.642244577407837\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 9/37 [01:29<04:36,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 9/37 [01:29<04:36,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.3006362915039062\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 10/37 [01:39<04:26,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 10/37 [01:39<04:26,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 10/37 [01:39<04:26,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 10/37 [01:39<04:26,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.2781819105148315\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 11/37 [01:49<04:17,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 11/37 [01:49<04:17,  9.89s/it]#015Training Epoch0:  30%|#033[34m██▉       #033[0m| 11/37 [01:49<04:17,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 11/37 [01:49<04:17,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 12/37 [01:59<04:07,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 12/37 [01:59<04:07,  9.90s/it]#015Training Epoch0:  32%|#033[34m███▏      #033[0m| 12/37 [01:59<04:07,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.3426694869995117\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 12/37 [01:59<04:07,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 13/37 [02:09<03:58,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 13/37 [02:09<03:58,  9.93s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.5526610612869263\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 13/37 [02:09<03:58,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 13/37 [02:09<03:58,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 14/37 [02:19<03:48,  9.95s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.3591638803482056\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 14/37 [02:19<03:48,  9.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 14/37 [02:19<03:48,  9.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 14/37 [02:19<03:48,  9.95s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.4422886371612549\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 15/37 [02:29<03:38,  9.92s/it]#015Training Epoch0:  41%|#033[34m████      #033[0m| 15/37 [02:29<03:38,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 15/37 [02:29<03:38,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 15/37 [02:29<03:38,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.1626042127609253\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 16/37 [02:38<03:28,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 16/37 [02:39<03:28,  9.91s/it]#015Training Epoch0:  43%|#033[34m████▎     #033[0m| 16/37 [02:39<03:28,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 16/37 [02:39<03:28,  9.91s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.2092169523239136\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 17/37 [02:48<03:17,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 17/37 [02:48<03:17,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 17/37 [02:48<03:17,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 17/37 [02:48<03:17,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.2522859573364258\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 18/37 [02:58<03:07,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 18/37 [02:58<03:07,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 18/37 [02:58<03:08,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 18/37 [02:58<03:08,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 19/37 [03:08<02:57,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 19/37 [03:08<02:57,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 19/37 [03:08<02:57,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.342434287071228\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 19/37 [03:08<02:57,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.2389991283416748\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 20/37 [03:18<02:48,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 20/37 [03:18<02:48,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 20/37 [03:18<02:48,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 20/37 [03:18<02:48,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.434442400932312\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 21/37 [03:28<02:38,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 21/37 [03:28<02:38,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 21/37 [03:28<02:38,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 21/37 [03:28<02:38,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.159277319908142\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 22/37 [03:38<02:28,  9.88s/it]#015Training Epoch0:  59%|#033[34m█████▉    #033[0m| 22/37 [03:38<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 22/37 [03:38<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 22/37 [03:38<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.4376779794692993\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 23/37 [03:48<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 23/37 [03:48<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 23/37 [03:48<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 23/37 [03:48<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 24/37 [03:57<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.198069453239441\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 24/37 [03:58<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 24/37 [03:58<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 24/37 [03:58<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.0581212043762207\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 25/37 [04:08<01:58,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 25/37 [04:07<01:58,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 25/37 [04:08<01:58,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 25/37 [04:08<01:58,  9.91s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.1952595710754395\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 26/37 [04:17<01:48,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 26/37 [04:17<01:48,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 26/37 [04:17<01:48,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 26/37 [04:17<01:48,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.4303056001663208\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 27/37 [04:27<01:39,  9.94s/it]#015Training Epoch0:  73%|#033[34m███████▎  #033[0m| 27/37 [04:28<01:39,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 27/37 [04:27<01:39,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 27/37 [04:27<01:39,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 28/37 [04:37<01:29,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 28/37 [04:37<01:29,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.6024481058120728\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 28/37 [04:37<01:29,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 28/37 [04:37<01:29,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 29/37 [04:47<01:19,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 29/37 [04:47<01:19,  9.90s/it]#015Training Epoch0:  78%|#033[34m███████▊  #033[0m| 29/37 [04:47<01:19,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.296597957611084\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 29/37 [04:47<01:19,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 30/37 [04:57<01:09,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 30/37 [04:57<01:09,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 30/37 [04:57<01:09,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.1429792642593384\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 30/37 [04:57<01:09,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 31/37 [05:07<00:59,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 1.2557381391525269\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 31/37 [05:07<00:59,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 31/37 [05:07<00:59,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 31/37 [05:07<00:59,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▋ #033[0m| 32/37 [05:17<00:49,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▋ #033[0m| 32/37 [05:17<00:49,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▋ #033[0m| 32/37 [05:17<00:49,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.1840070486068726\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▋ #033[0m| 32/37 [05:17<00:49,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 33/37 [05:27<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.1840932369232178\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 33/37 [05:27<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 33/37 [05:27<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 33/37 [05:27<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 34/37 [05:37<00:29,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 34/37 [05:37<00:29,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.1589747667312622\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 34/37 [05:37<00:29,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 34/37 [05:36<00:29,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 35/37 [05:47<00:19,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 35/37 [05:46<00:19,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 35/37 [05:46<00:19,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 1.254852533340454\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 35/37 [05:46<00:19,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 36/37 [05:56<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 36/37 [05:56<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.1636501550674438\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 36/37 [05:56<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 36/37 [05:56<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 37/37 [06:06<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.4642081260681152\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 37/37 [06:06<00:00,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 37/37 [06:06<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 37/37 [06:06<00:00,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 37/37 [06:06<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 37/37 [06:06<00:00,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 37/37 [06:06<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 37/37 [06:06<00:00,  9.91s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/38 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/38 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/38 [00:00<?, ?it/s]#015evaluating Epoch:   0%|#033[32m          #033[0m| 0/38 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/38 [00:03<02:13,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/38 [00:03<02:14,  3.63s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/38 [00:03<02:13,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/38 [00:03<02:14,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 2/38 [00:07<02:08,  3.56s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 2/38 [00:07<02:08,  3.56s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 2/38 [00:07<02:08,  3.56s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 2/38 [00:07<02:08,  3.56s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 3/38 [00:10<02:03,  3.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 3/38 [00:10<02:03,  3.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 3/38 [00:10<02:03,  3.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 3/38 [00:10<02:03,  3.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 4/38 [00:14<01:59,  3.52s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 4/38 [00:14<01:59,  3.52s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 4/38 [00:14<01:59,  3.52s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 4/38 [00:14<01:59,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 5/38 [00:17<01:55,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 5/38 [00:17<01:55,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 5/38 [00:17<01:55,  3.51s/it]#015evaluating Epoch:  13%|#033[32m█▎        #033[0m| 5/38 [00:17<01:55,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 6/38 [00:21<01:52,  3.51s/it]#015evaluating Epoch:  16%|#033[32m█▌        #033[0m| 6/38 [00:21<01:52,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 6/38 [00:21<01:52,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 6/38 [00:21<01:52,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 7/38 [00:24<01:48,  3.50s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 7/38 [00:24<01:48,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 7/38 [00:24<01:48,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 7/38 [00:24<01:48,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 8/38 [00:28<01:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 8/38 [00:28<01:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 8/38 [00:28<01:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 8/38 [00:28<01:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▎       #033[0m| 9/38 [00:31<01:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▎       #033[0m| 9/38 [00:31<01:41,  3.50s/it]#015evaluating Epoch:  24%|#033[32m██▎       #033[0m| 9/38 [00:31<01:41,  3.50s/it]#015evaluating Epoch:  24%|#033[32m██▎       #033[0m| 9/38 [00:31<01:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 10/38 [00:35<01:38,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 10/38 [00:35<01:38,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 10/38 [00:35<01:38,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 10/38 [00:35<01:38,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 11/38 [00:38<01:34,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 11/38 [00:38<01:34,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 11/38 [00:38<01:34,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 11/38 [00:38<01:34,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 12/38 [00:42<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 12/38 [00:42<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 12/38 [00:42<01:31,  3.50s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 12/38 [00:42<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 13/38 [00:45<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 13/38 [00:45<01:27,  3.50s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 13/38 [00:45<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 13/38 [00:45<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 14/38 [00:49<01:23,  3.50s/it]#015evaluating Epoch:  37%|#033[32m███▋      #033[0m| 14/38 [00:49<01:23,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 14/38 [00:49<01:23,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 14/38 [00:49<01:23,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 15/38 [00:52<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 15/38 [00:52<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 15/38 [00:52<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 15/38 [00:52<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 16/38 [00:56<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 16/38 [00:56<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 16/38 [00:56<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 16/38 [00:56<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 17/38 [00:59<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 17/38 [00:59<01:13,  3.49s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 17/38 [00:59<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 17/38 [00:59<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 18/38 [01:03<01:09,  3.49s/it]#015evaluating Epoch:  47%|#033[32m████▋     #033[0m| 18/38 [01:03<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 18/38 [01:03<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 18/38 [01:03<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 19/38 [01:06<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 19/38 [01:06<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 19/38 [01:06<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 19/38 [01:06<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 20/38 [01:10<01:02,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 20/38 [01:10<01:02,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 20/38 [01:10<01:02,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 20/38 [01:10<01:02,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 21/38 [01:13<00:59,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 21/38 [01:13<00:59,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 21/38 [01:13<00:59,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 21/38 [01:13<00:59,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 22/38 [01:17<00:56,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 22/38 [01:17<00:56,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 22/38 [01:17<00:56,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 22/38 [01:17<00:56,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 23/38 [01:20<00:52,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 23/38 [01:20<00:52,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 23/38 [01:20<00:52,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 23/38 [01:20<00:52,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 24/38 [01:24<00:49,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 24/38 [01:24<00:49,  3.50s/it]#015evaluating Epoch:  63%|#033[32m██████▎   #033[0m| 24/38 [01:24<00:49,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 24/38 [01:24<00:49,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 25/38 [01:27<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 25/38 [01:27<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 25/38 [01:27<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 25/38 [01:27<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 26/38 [01:31<00:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 26/38 [01:31<00:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 26/38 [01:31<00:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 26/38 [01:31<00:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 27/38 [01:34<00:38,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 27/38 [01:34<00:38,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 27/38 [01:34<00:38,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 27/38 [01:34<00:38,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 28/38 [01:38<00:35,  3.51s/it]#015evaluating Epoch:  74%|#033[32m███████▎  #033[0m| 28/38 [01:38<00:35,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 28/38 [01:38<00:35,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 28/38 [01:38<00:35,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 29/38 [01:41<00:31,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 29/38 [01:41<00:31,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 29/38 [01:41<00:31,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 29/38 [01:41<00:31,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 30/38 [01:45<00:28,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 30/38 [01:45<00:28,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 30/38 [01:45<00:28,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 30/38 [01:45<00:28,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 31/38 [01:48<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 31/38 [01:48<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 31/38 [01:48<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 31/38 [01:48<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 32/38 [01:52<00:21,  3.50s/it]#015evaluating Epoch:  84%|#033[32m████████▍ #033[0m| 32/38 [01:52<00:21,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 32/38 [01:52<00:21,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 32/38 [01:52<00:21,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 33/38 [01:55<00:17,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 33/38 [01:55<00:17,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 33/38 [01:55<00:17,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 33/38 [01:55<00:17,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 34/38 [01:59<00:14,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 34/38 [01:59<00:14,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 34/38 [01:59<00:14,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 34/38 [01:59<00:14,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 35/38 [02:02<00:10,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 35/38 [02:02<00:10,  3.51s/it]#015evaluating Epoch:  92%|#033[32m█████████▏#033[0m| 35/38 [02:02<00:10,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 35/38 [02:02<00:10,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 36/38 [02:06<00:07,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 36/38 [02:06<00:07,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 36/38 [02:06<00:07,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 36/38 [02:06<00:07,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 37/38 [02:09<00:03,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 37/38 [02:09<00:03,  3.50s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 37/38 [02:09<00:03,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 37/38 [02:09<00:03,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 38/38 [02:13<00:00,  3.50s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 38/38 [02:13<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 38/38 [02:13<00:00,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 38/38 [02:13<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 38/38 [02:13<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 38/38 [02:13<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 38/38 [02:13<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 38/38 [02:13<00:00,  3.51s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.4664, device='cuda:0') eval_epoch_loss=tensor(1.2431, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.2431223392486572\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=3.8180, train_epoch_loss=1.3397, epcoh time 367.03074057599997s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 3.8179757595062256\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.339720368385315\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 3.4664199352264404\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.2431223392486572\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 367.03074057599997\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 3.482137368999929\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.63it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.24it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-01-20 10:36:07,630 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-20 10:36:07,630 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-20 10:36:07,631 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-01-20 10:36:09 Uploading - Uploading generated training model\n",
      "2024-01-20 10:37:16 Completed - Training job completed\n",
      "Training seconds: 1140\n",
      "Billable seconds: 1140\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    instance_count=1,\n",
    "    environment={\"accept_eula\": \"true\"}\n",
    ")\n",
    "\n",
    "# By default, instruction tuning is set to false\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", \n",
    "                              epoch=\"1\", \n",
    "                              max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2024-01-20-11-22-57-257\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2024-01-20-11-22-57-254\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2024-01-20-11-22-57-254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Response from non-finetuned model</th>\n",
       "      <th>Response from fine-tuned model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### Instruction What is Milky Way?\\n\\n### Context\\nThe Milky Way is the galaxy that includes the Solar System, with the name describing the galaxy's appearance from Earth: a hazy band of light seen in the night sky formed from stars that cannot be individually distinguished by the naked eye. The term Milky Way is a translation of the Latin via lactea, from the Greek γαλακτικὸς κύκλος (galaktikòs kýklos), meaning \"milky circle\". From Earth, the Milky Way appears as a band because its disk-shaped structure is viewed from within. Galileo Galilei first resolved the band of light into individual stars with his telescope in 1610. Until the early 1920s, most astronomers thought that the Milky Way contained all the stars in the Universe. Following the 1920 Great Debate between the astronomers Harlow Shapley and Heber Doust Curtis, observations by Edwin Hubble showed that the Milky Way is just one of many galaxies.\\n\\n### Response:\\n</td>\n",
       "      <td>Milky Way is the galaxy that includes Solar System in which our planet Earth is located.</td>\n",
       "      <td>\\n* I can’t see anything\\n\\n### Instruction What is The Big Bang?\\n\\n### Context\\n\\nThe Big Bang theory is an early cosmological model of the universe based on the general theory of relativity, which states that space and time emerged together 13.8 billion years ago with a singularity. The Big Bang theory is the prevailing cosmological model for the universe from the earliest known periods through its subsequent large-scale evolution.[1</td>\n",
       "      <td>The Milky Way galaxy is an example of a barred spiral galaxy. It has a bulge at its center and two spiral arms that wind outward with logarithmic patterns toward the poles of the galaxy. The Milky Way is the galaxy that includes the Solar System, with the name describing the galaxy's appearance from Earth: a hazy band of light seen in the night sky formed from stars that cannot be individually distinguished by the naked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### Instruction What is the capital of Mongolia?\\n\\n### Context\\nMongolia (/mɒŋˈɡoʊliə/ (listen)) is a landlocked country in East Asia, bordered by Russia to the north and China to the south. It covers an area of 1,564,116 square kilometres (603,909 square miles), with a population of just 3.3 million, making it the world's most sparsely populated sovereign nation. Mongolia is the world's largest landlocked country that does not border a closed sea, and much of its area is covered by grassy steppe, with mountains to the north and west and the Gobi Desert to the south. Ulaanbaatar, the capital and largest city, is home to roughly half of the country's population.\\n\\n### Response:\\n</td>\n",
       "      <td>Ulaanbaatar is the capital of Mongolia</td>\n",
       "      <td>\\n* Mongolia's capital is Ulan Bator.\\n\\n## Answer 2\\n\\nWould you like to fly on an Air France flights?\\n\\n### Instruction\\n\\n### Context\\nAir France (AF, originally Société Air France) is a French airline headquartered in Tremblay-en-France. It is a subsidiary of Air France–KLM created after the 2004 French Airline Indust</td>\n",
       "      <td>The capital of Mongolia is Ulaanbaatar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### Instruction Here is some reference information about Huaquechula, When does the feast of the Holy Cross in Huaquechula start?\\n\\n### Context\\nOne major festival is that of the Feast of the Holy Cross, which dates back far into the colonial period. It has a community cross made from basalt called the “Cruz de Huaquechula” which is incrusted with various relics related to the original cross of Christ. It is said to be so heavy that it cannot be lifted unless the bearers pray and the church plays a special melody on its bells. The festival lasts nine days beginning in April ending on May 3, with mass, traditional dance, food and music played by bands playing wind instruments, and fireworks set off from large frames in the shape bulls. The event has been named part of the Cultural Heritage of Puebla. During this event, traditional festive wear such as that of the charro and the China Poblana can be seen.\\n\\n### Response:\\n</td>\n",
       "      <td>April 25th.  It ends on May 3rd, and lasts 9 days.</td>\n",
       "      <td>### Huaquechula: Community Cross\\n\\nThe community cross in the 21-meter-tall frame is carried by the residents on May 3 to San Felipe Cathedral, where it is placed above the main altar for veneration. It is said that an apparation of the Holy Cross appeared in 2003, and a woman by the name of Sra. Estrella was ordered to build a frame for it.\\nAcc</td>\n",
       "      <td>The festival starts in April and lasts 9 days ending on May 3rd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### Instruction Did the dome last as long as it was designed for?\\n\\n### Context\\nAfter an ice storm in December 2015, the 13-year-old dome collapsed. It housed several events for indoor track athletes, including practices and meets, practice room for all winter sports teams, physical education classes, and various community events.\\n\\nThe dome was designed by Air Structures American Technologies Inc. and cost $3 million at the time of its construction in 2002. It was meant to last 15–20 years.\\n\\n### Response:\\n</td>\n",
       "      <td>No, it did not. The dome only lasted 13 years, but it was designed to last 15-20 years.</td>\n",
       "      <td>The dome collapsed due to the ice storm in 2015.\\n\\nThe university started fundraising for repairs on December 22, 2015.\\n\\nAir Structures American Technologies Inc. provided warranty service to the school, helping with the insurance claims.\\n</td>\n",
       "      <td>Yes, it did.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### Instruction What year was Kerala formed?\\n\\n### Context\\nKerala (English: /ˈkɛrələ/ KERR-ə-lə; Malayalam: [ke:ɾɐɭɐm] (listen)) is a state on the Malabar Coast of India. It was formed on 1 November 1956, following the passage of the States Reorganisation Act, by combining Malayalam-speaking regions of the erstwhile regions of Cochin, Malabar, South Canara, and Travancore. Spread over 38,863 km2 (15,005 sq mi), Kerala is the 21st largest Indian state by area. It is bordered by Karnataka to the north and northeast, Tamil Nadu to the east and south, and the Lakshadweep Sea to the west. With 33 million inhabitants as per the 2011 census, Kerala is the 13th-largest Indian state by population. It is divided into 14 districts with the capital being Thiruvananthapuram. Malayalam is the most widely spoken language and is also the official language of the state.\\n\\n### Response:\\n</td>\n",
       "      <td>1956</td>\n",
       "      <td>1044-1045(12)\\n\\n### Explanation\\n1956\\n```{discussion}\\n\\n```</td>\n",
       "      <td>It was formed in 1956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    instruction = f\"### Instruction {datapoint['instruction']}\"\n",
    "    context = f\"### Context\\n{datapoint['context']}\" \n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context] if i is not None])\n",
    "    \n",
    "    payload = {\n",
    "        #\"inputs\": template[\"prompt\"].format(\n",
    "        \"inputs\": prompt.format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generation\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generation\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-2-7b-2024-01-20-10-00-02-544\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-2-7b-2024-01-20-10-00-02-697\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-2-7b-2024-01-20-10-00-02-697\n",
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-2-7b-2024-01-20-11-22-57-257\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-2-7b-2024-01-20-11-22-57-254\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-2-7b-2024-01-20-11-22-57-254\n"
     ]
    }
   ],
   "source": [
    "# Delete resources\n",
    "pretrained_predictor.delete_model()\n",
    "pretrained_predictor.delete_endpoint()\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What to do next\n",
    "- Understand the various parameters in finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35a3af-c45d-4f27-bbc6-af2c3e8bd49f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Appendix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- epoch: The number of passes that the fine-tuning algorithm takes through the training dataset. Must be an integer greater than 1. Default: 5\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 1e-4.\n",
    "- instruction_tuned: Whether to instruction-train the model or not. Must be 'True' or 'False'. Default: 'False'\n",
    "- per_device_train_batch_size: The batch size per GPU core/CPU for training. Must be a positive integer. Default: 4.\n",
    "- per_device_eval_batch_size: The batch size per GPU core/CPU for evaluation. Must be a positive integer. Default: 1\n",
    "- max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this value. Value -1 means using all of training samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this value. Value -1 means using all of validation samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. If -1, max_input_length is set to the minimum of 1024 and the maximum model length defined by the tokenizer. If set to a positive value, max_input_length is set to the minimum of the provided value and the model_max_length defined by the tokenizer. Must be a positive integer or -1. Default: -1. \n",
    "- validation_split_ratio: If validation channel is none, ratio of train-validation split from the train data. Must be between 0 and 1. Default: 0.2. \n",
    "- train_data_split_seed: If validation data is not present, this fixes the random splitting of the input training data to training and validation data used by the algorithm. Must be an integer. Default: 0.\n",
    "- preprocessing_num_workers: The number of processes to use for the preprocessing. If None, main process is used for preprocessing. Default: \"None\"\n",
    "- lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "- lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "- lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "- int8_quantization: If True, model is loaded with 8 bit precision for training. Default for 7B/13B: False. Default for 70B: True.\n",
    "- enable_fsdp: If True, training uses Fully Sharded Data Parallelism. Default for 7B/13B: True. Default for 70B: False.\n",
    "\n",
    "Note 1: int8_quantization is not supported with FSDP. Also, int8_quantization = 'False' and enable_fsdp = 'False' is not supported due to CUDA memory issues for any of the g5 family instances. Thus, we recommend setting exactly one of int8_quantization or enable_fsdp to be 'True'\n",
    "Note 2: Due to the size of the model, 70B model can not be fine-tuned with enable_fsdp = 'True' for any of the supported instance types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6d023-3487-4571-8b52-f332790c1ad7",
   "metadata": {},
   "source": [
    "### Supported Instance types\n",
    "\n",
    "---\n",
    "We have tested our scripts on the following instances types:\n",
    "\n",
    "- 7B: ml.g5.12xlarge, nl.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 13B: ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 70B: ml.g5.48xlarge\n",
    "\n",
    "Other instance types may also work to fine-tune. Note: When using p3 instances, training will be done with 32 bit precision as bfloat16 is not supported on these instances. Thus, training job would consume double the amount of CUDA memory when training on p3 instances compared to g5 instances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d4350-cf4d-40a0-be1c-eba44efd33ab",
   "metadata": {},
   "source": [
    "### Few notes about the fine-tuning method\n",
    "\n",
    "---\n",
    "- Fine-tuning scripts are based on [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). \n",
    "- Instruction tuning dataset is first converted into domain adaptation dataset format before fine-tuning. \n",
    "- Fine-tuning scripts utilize Fully Sharded Data Parallel (FSDP) as well as Low Rank Adaptation (LoRA) method fine-tuning the models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd6dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
