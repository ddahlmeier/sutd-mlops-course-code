{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Fine-tune LLaMA 2 models on SageMaker JumpStart\n",
    "From https://github.com/generative-ai-on-aws/generative-ai-on-aws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9539dd-ad5e-4fd9-bc65-b75dc0ea4db0",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b674854d-4a29-4737-afe0-d8b9b530e71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets[s3] in /opt/conda/lib/python3.10/site-packages (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (1.26.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets[s3]) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (0.19.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (6.0.1)\n",
      "Collecting s3fs (from datasets[s3])\n",
      "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets[s3]) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets[s3]) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[s3]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[s3]) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[s3]) (2023.3)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.10/site-packages (from s3fs->datasets[s3]) (2.7.0)\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading s3fs-2023.12.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.9.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.5.4 (from s3fs->datasets[s3])\n",
      "  Downloading aiobotocore-2.5.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting s3fs (from datasets[s3])\n",
      "  Downloading s3fs-2023.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting botocore<1.31.18,>=1.31.17 (from aiobotocore~=2.5.4->s3fs->datasets[s3])\n",
      "  Downloading botocore-1.31.17-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.5.4->s3fs->datasets[s3]) (1.15.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.5.4->s3fs->datasets[s3]) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets[s3]) (1.16.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.4->s3fs->datasets[s3]) (1.0.1)\n",
      "Downloading s3fs-2023.6.0-py3-none-any.whl (28 kB)\n",
      "Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.64\n",
      "    Uninstalling botocore-1.31.64:\n",
      "      Successfully uninstalled botocore-1.31.64\n",
      "  Attempting uninstall: aiobotocore\n",
      "    Found existing installation: aiobotocore 2.7.0\n",
      "    Uninstalling aiobotocore-2.7.0:\n",
      "      Successfully uninstalled aiobotocore-2.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "amazon-sagemaker-jupyter-scheduler 3.0.4 requires aiobotocore==2.7.*, but you have aiobotocore 2.5.4 which is incompatible.\n",
      "boto3 1.28.64 requires botocore<1.32.0,>=1.31.64, but you have botocore 1.31.17 which is incompatible.\n",
      "sagemaker-jupyterlab-extension-common 0.1.9 requires aiobotocore>=2.7.0, but you have aiobotocore 2.5.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.5.4 botocore-1.31.17 s3fs-2023.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets[s3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "---\n",
    "\n",
    "First we will deploy the Llama-2 model as a SageMaker endpoint and do some inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97e1d98f",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"2.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For forward compatibility, pin to model_version='2.*' in your JumpStartModel or JumpStartEstimator definitions. Note that major version upgrades may have different EULA acceptance terms and input/output signatures.\n",
      "For forward compatibility, pin to model_version='2.*' in your JumpStartModel or JumpStartEstimator definitions. Note that major version upgrades may have different EULA acceptance terms and input/output signatures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd833f8-1ddc-4805-80b2-19e7db629880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to be happy.\n",
      "I’m a huge believer in the power of positive thinking, and I try to live my life by that philosophy. I think that if you can find happiness in the little things, then you can find happiness in the big things.\n",
      "I believe that we all have the power to\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = pretrained_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e16f-d459-40c6-9d6b-0272938b3878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. We will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48573a6548fb4f4faefcf272de81a963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What name is Beijing also known by?',\n",
       " 'context': \"Beijing (/beɪˈdʒɪŋ/ bay-JING; Chinese: 北京; pinyin: Běijīng; Mandarin pronunciation: [pèɪ.tɕíŋ] (listen)), alternatively romanized as Peking (/piːˈkɪŋ/ pee-KING), is the capital of the People's Republic of China. With over 21 million residents, Beijing is the world's most populous national capital city and is China's second largest city after Shanghai. It is located in Northern China, and is governed as a municipality under the direct administration of the State Council with 16 urban, suburban, and rural districts. Beijing is mostly surrounded by Hebei Province with the exception of neighboring Tianjin to the southeast; together, the three divisions form the Jingjinji megalopolis and the national capital region of China.\",\n",
       " 'response': \"Běijīng is alternatively romanized as Peking and is the capital of the People's Republic of China\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load smaller dataset for faster training\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train[:10%]\")\n",
    "\n",
    "# Filter for question answering examples\n",
    "qa_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"closed_qa\")\n",
    "qa_dataset = qa_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = qa_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_and_test_dataset[\"train\"]\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_dataset.to_json(\"training.jsonl\")\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we create a prompt template for using the data in an instruction / input format for the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# TODO what is the right template?\n",
    "# template = {\n",
    "#     \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "#     \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "#     \"### Instruction:\\n{instruction}\\n\\n### Context:\\n{context}\\n\\n\",\n",
    "#     \"Answer\": \" {response}\",\n",
    "# }\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \"{response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92ac02d1-3a59-4779-8a43-42262302bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Training data: s3://sagemaker-us-east-1-703877312554/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"training.jsonl\"\n",
    "train_data_location = f\"s3://{bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the model\n",
    "\n",
    "Next, we fine-tune the LLaMA v2 7B model. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2024-01-01-09-40-22-349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01 09:40:22 Starting - Starting the training job...\n",
      "2024-01-01 09:40:50 Starting - Preparing the instances for training..........................................\n",
      "2024-01-01 09:47:28 Downloading - Downloading input data..................\n",
      "2024-01-01 09:50:38 Downloading - Downloading the training image............\n",
      "2024-01-01 09:52:49 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-01 09:53:39,566 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-01 09:53:39,627 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 09:53:39,636 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-01 09:53:39,637 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-01 09:53:47,558 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=4e5810eeeaa607001b4e7454aadda2d72835db846f75601e86d3497ec8b80392\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-01-01 09:54:42,206 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-01 09:54:42,206 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-01 09:54:42,277 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 09:54:42,339 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 09:54:42,400 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 09:54:42,408 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"1\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2024-01-01-09-40-22-349\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"1\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"1\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2024-01-01-09-40-22-349\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"1\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=1\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 1 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-01-01 09:54:42,436 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '1', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2024-01-01 09:54:47,757] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-01-01 09:54:47,757] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-01-01 09:54:47,757] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-01-01 09:54:47,757] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1279.53it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mGenerating train split: 1218 examples [00:00, 62203.66 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:00<00:00, 17235.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:00<00:00, 17370.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:00<00:00, 17495.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:00<00:00, 17714.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 1000/1218 [00:02<00:00, 384.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 1000/1218 [00:02<00:00, 385.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 1000/1218 [00:02<00:00, 385.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 1000/1218 [00:02<00:00, 381.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:03<00:00, 410.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:03<00:00, 403.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:03<00:00, 411.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:03<00:00, 404.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:03<00:00, 411.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:03<00:00, 404.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:03<00:00, 407.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:03<00:00, 400.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1218 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 1000/1218 [00:00<00:00, 1771.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 1000/1218 [00:00<00:00, 1637.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 1000/1218 [00:00<00:00, 1770.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 1000/1218 [00:00<00:00, 1689.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:00<00:00, 1833.79 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:00<00:00, 1712.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:00<00:00, 1844.22 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:00<00:00, 1782.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1218/1218 [00:00<00:00, 1754.02 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.69s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.01s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 492\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 124\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda11.8\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.7172422409057617\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/30 [00:10<05:04, 10.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/30 [00:10<05:04, 10.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/30 [00:10<05:05, 10.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 1/30 [00:10<05:04, 10.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/30 [00:20<04:43, 10.13s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/30 [00:20<04:43, 10.14s/it]#015Training Epoch0:   7%|#033[34m▋         #033[0m| 2/30 [00:20<04:43, 10.12s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.834979772567749\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/30 [00:20<04:43, 10.13s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.6246498823165894\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 3/30 [00:30<04:30, 10.00s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 3/30 [00:30<04:29, 10.00s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 3/30 [00:30<04:30, 10.01s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 3/30 [00:30<04:30, 10.00s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.4383846521377563\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 4/30 [00:40<04:18,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 4/30 [00:40<04:18,  9.94s/it]#015Training Epoch0:  13%|#033[34m█▎        #033[0m| 4/30 [00:40<04:18,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 4/30 [00:40<04:18,  9.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 5/30 [00:49<04:07,  9.91s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.8449327945709229\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 5/30 [00:49<04:07,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 5/30 [00:49<04:07,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 5/30 [00:49<04:07,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 6/30 [00:59<03:57,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.1870838403701782\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 6/30 [00:59<03:57,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 6/30 [00:59<03:57,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 6/30 [00:59<03:57,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 7/30 [01:09<03:47,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 7/30 [01:09<03:47,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 7/30 [01:09<03:47,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.6466171741485596\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 7/30 [01:09<03:47,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 8/30 [01:19<03:37,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 8/30 [01:19<03:37,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.6548007726669312\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 8/30 [01:19<03:37,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 8/30 [01:19<03:37,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 9/30 [01:29<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 9/30 [01:29<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.476121187210083\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 9/30 [01:29<03:27,  9.87s/it]#015Training Epoch0:  30%|#033[34m███       #033[0m| 9/30 [01:29<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 10/30 [01:39<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.5098621845245361\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 10/30 [01:39<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 10/30 [01:39<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 10/30 [01:39<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 11/30 [01:49<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 11/30 [01:49<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.398754596710205\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 11/30 [01:49<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 11/30 [01:49<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 12/30 [01:58<02:57,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.221560001373291\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 12/30 [01:58<02:57,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 12/30 [01:58<02:57,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 12/30 [01:58<02:57,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 13/30 [02:08<02:48,  9.90s/it]#015Training Epoch0:  43%|#033[34m████▎     #033[0m| 13/30 [02:08<02:48,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 13/30 [02:08<02:48,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.4777741432189941\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 13/30 [02:08<02:48,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 14/30 [02:18<02:38,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 14/30 [02:18<02:38,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 14/30 [02:18<02:38,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.449379324913025\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 14/30 [02:18<02:38,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 15/30 [02:28<02:28,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 15/30 [02:28<02:28,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.4600825309753418\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 15/30 [02:28<02:28,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 15/30 [02:28<02:28,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.340528130531311\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 16/30 [02:38<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 16/30 [02:38<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 16/30 [02:38<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 16/30 [02:38<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.2359578609466553\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 17/30 [02:48<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 17/30 [02:48<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 17/30 [02:48<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 17/30 [02:48<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.1335322856903076\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 18/30 [02:58<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 18/30 [02:58<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 18/30 [02:58<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 18/30 [02:58<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 19/30 [03:08<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 19/30 [03:08<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.0887612104415894\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 19/30 [03:08<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 19/30 [03:08<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.22037672996521\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 20/30 [03:17<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 20/30 [03:18<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 20/30 [03:17<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 20/30 [03:17<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.258829951286316\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 21/30 [03:27<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 21/30 [03:27<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 21/30 [03:27<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 21/30 [03:27<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.3997046947479248\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 22/30 [03:37<01:18,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 22/30 [03:37<01:18,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 22/30 [03:37<01:18,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 22/30 [03:37<01:18,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 23/30 [03:47<01:08,  9.85s/it]#015Training Epoch0:  77%|#033[34m███████▋  #033[0m| 23/30 [03:47<01:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 23/30 [03:47<01:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.4217654466629028\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 23/30 [03:47<01:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.5101388692855835\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 24/30 [03:57<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 24/30 [03:57<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 24/30 [03:57<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 24/30 [03:57<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.2664250135421753\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 25/30 [04:07<00:49,  9.86s/it]#015Training Epoch0:  83%|#033[34m████████▎ #033[0m| 25/30 [04:07<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 25/30 [04:07<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 25/30 [04:07<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 26/30 [04:17<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.497304081916809\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 26/30 [04:17<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 26/30 [04:17<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 26/30 [04:17<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.306596040725708\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 27/30 [04:27<00:29,  9.91s/it]#015Training Epoch0:  90%|#033[34m█████████ #033[0m| 27/30 [04:27<00:29,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 27/30 [04:27<00:29,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 27/30 [04:27<00:29,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 28/30 [04:36<00:19,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 28/30 [04:37<00:19,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 28/30 [04:37<00:19,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.31233549118042\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 28/30 [04:37<00:19,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.3920238018035889\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 29/30 [04:46<00:09,  9.89s/it]#015Training Epoch0:  97%|#033[34m█████████▋#033[0m| 29/30 [04:46<00:09,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 29/30 [04:46<00:09,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 29/30 [04:46<00:09,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 30/30 [04:56<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.4600764513015747\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 30/30 [04:56<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[34m#015Training Epoch0: 100%|#033[34m██████████#033[0m| 30/30 [04:56<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 30/30 [04:56<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 30/30 [04:56<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 30/30 [04:56<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 30/30 [04:56<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 30/30 [04:56<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/31 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/31 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/31 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/31 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/31 [00:03<01:48,  3.61s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/31 [00:03<01:48,  3.61s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/31 [00:03<01:48,  3.61s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/31 [00:03<01:48,  3.60s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 2/31 [00:07<01:42,  3.54s/it]#015evaluating Epoch:   6%|#033[32m▋         #033[0m| 2/31 [00:07<01:42,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 2/31 [00:07<01:42,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 2/31 [00:07<01:42,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 3/31 [00:10<01:38,  3.52s/it]#015evaluating Epoch:  10%|#033[32m▉         #033[0m| 3/31 [00:10<01:38,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 3/31 [00:10<01:38,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 3/31 [00:10<01:38,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 4/31 [00:14<01:34,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 4/31 [00:14<01:34,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 4/31 [00:14<01:34,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 4/31 [00:14<01:34,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 5/31 [00:17<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 5/31 [00:17<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 5/31 [00:17<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 5/31 [00:17<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 6/31 [00:21<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 6/31 [00:21<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 6/31 [00:21<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 6/31 [00:21<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 7/31 [00:24<01:23,  3.50s/it]#015evaluating Epoch:  23%|#033[32m██▎       #033[0m| 7/31 [00:24<01:23,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 7/31 [00:24<01:23,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 7/31 [00:24<01:23,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 8/31 [00:28<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 8/31 [00:28<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 8/31 [00:28<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 8/31 [00:28<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 9/31 [00:31<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 9/31 [00:31<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 9/31 [00:31<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 9/31 [00:31<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 10/31 [00:35<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 10/31 [00:35<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 10/31 [00:35<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 10/31 [00:35<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 11/31 [00:38<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 11/31 [00:38<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 11/31 [00:38<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 11/31 [00:38<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▊      #033[0m| 12/31 [00:42<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▊      #033[0m| 12/31 [00:42<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▊      #033[0m| 12/31 [00:42<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▊      #033[0m| 12/31 [00:42<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 13/31 [00:45<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 13/31 [00:45<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 13/31 [00:45<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 13/31 [00:45<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 14/31 [00:49<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 14/31 [00:48<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 14/31 [00:48<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 14/31 [00:49<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 15/31 [00:52<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 15/31 [00:52<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 15/31 [00:52<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 15/31 [00:52<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 16/31 [00:55<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 16/31 [00:55<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 16/31 [00:55<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 16/31 [00:55<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 17/31 [00:59<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 17/31 [00:59<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 17/31 [00:59<00:48,  3.49s/it]#015evaluating Epoch:  55%|#033[32m█████▍    #033[0m| 17/31 [00:59<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 18/31 [01:02<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 18/31 [01:02<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 18/31 [01:02<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 18/31 [01:02<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████▏   #033[0m| 19/31 [01:06<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████▏   #033[0m| 19/31 [01:06<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████▏   #033[0m| 19/31 [01:06<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████▏   #033[0m| 19/31 [01:06<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 20/31 [01:09<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 20/31 [01:09<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 20/31 [01:09<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 20/31 [01:09<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 21/31 [01:13<00:34,  3.49s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 21/31 [01:13<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 21/31 [01:13<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 21/31 [01:13<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 22/31 [01:16<00:31,  3.49s/it]#015evaluating Epoch:  71%|#033[32m███████   #033[0m| 22/31 [01:16<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 22/31 [01:16<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 22/31 [01:16<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 23/31 [01:20<00:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 23/31 [01:20<00:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 23/31 [01:20<00:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 23/31 [01:20<00:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 24/31 [01:23<00:24,  3.49s/it]#015evaluating Epoch:  77%|#033[32m███████▋  #033[0m| 24/31 [01:23<00:24,  3.49s/it]#015evaluating Epoch:  77%|#033[32m███████▋  #033[0m| 24/31 [01:23<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 24/31 [01:23<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 25/31 [01:27<00:20,  3.49s/it]#015evaluating Epoch:  81%|#033[32m████████  #033[0m| 25/31 [01:27<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 25/31 [01:27<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 25/31 [01:27<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 26/31 [01:30<00:17,  3.50s/it]#015evaluating Epoch:  84%|#033[32m████████▍ #033[0m| 26/31 [01:30<00:17,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 26/31 [01:30<00:17,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 26/31 [01:30<00:17,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 27/31 [01:34<00:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 27/31 [01:34<00:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 27/31 [01:34<00:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 27/31 [01:34<00:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 28/31 [01:37<00:10,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 28/31 [01:37<00:10,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 28/31 [01:37<00:10,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 28/31 [01:37<00:10,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 29/31 [01:41<00:06,  3.49s/it]#015evaluating Epoch:  94%|#033[32m█████████▎#033[0m| 29/31 [01:41<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 29/31 [01:41<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 29/31 [01:41<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 30/31 [01:44<00:03,  3.49s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 30/31 [01:44<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 30/31 [01:44<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 30/31 [01:44<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 31/31 [01:48<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 31/31 [01:48<00:00,  3.49s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 31/31 [01:48<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 31/31 [01:48<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 31/31 [01:48<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 31/31 [01:48<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 31/31 [01:48<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 31/31 [01:48<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.6811, device='cuda:0') eval_epoch_loss=tensor(1.3032, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.303223729133606\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=4.0938, train_epoch_loss=1.4095, epcoh time 296.99738539999987s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 4.093808650970459\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.4094756841659546\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 3.6811444759368896\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.303223729133606\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 296.99738539999987\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 3.482358584999929\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.36it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.13it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.96it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-01-01 10:04:45,918 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-01 10:04:45,918 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-01 10:04:45,918 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-01-01 10:05:18 Uploading - Uploading generated training model\n",
      "2024-01-01 10:06:19 Completed - Training job completed\n",
      "Training seconds: 1132\n",
      "Billable seconds: 1132\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    instance_count=1,\n",
    "    environment={\"accept_eula\": \"true\"}\n",
    ")\n",
    "\n",
    "# By default, instruction tuning is set to false\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", \n",
    "                              epoch=\"1\", \n",
    "                              max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2024-01-01-10-12-26-073\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2024-01-01-10-12-26-069\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2024-01-01-10-12-26-069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Response from non-finetuned model</th>\n",
       "      <th>Response from fine-tuned model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### Instruction what are the 5 skandhas?\\n\\n### Context\\nSkandhas (Sanskrit) or khandhas (Pāḷi) means \"heaps, aggregates, collections, groupings\". In Buddhism, it refers to the five aggregates of clinging (Pañcupādānakkhandhā), the five material and mental factors that take part in the rise of craving and clinging. They are also explained as the five factors that constitute and explain a sentient being’s person and personality, but this is a later interpretation in response to sarvastivadin essentialism.\\n\\nThe five aggregates or heaps of clinging are:\\n\\n1. form (or material image, impression) (rupa)\\n2. sensations (or feelings, received from form) (vedana)\\n3. perceptions (samjna)\\n4. mental activity or formations (sankhara)\\n5. consciousness (vijnana).\\n\\n### Response:\\n</td>\n",
       "      <td>The five skhandas are form, sensations, perceptions, mental activity and consciousness</td>\n",
       "      <td>\\n#### 1. form (or material image, impression) (rupa)\\n\\n#### 2. sensations (or feelings, received from form) (vedana)\\n\\n#### 3. perceptions (samjna)\\n\\n#### 4. mental activity or formations (sankhara)\\n\\n#### 5. consciousness (vijnana)\\n\\n\\n### Reference:\\nhttps://en.wikipedia.org/wiki/Skand</td>\n",
       "      <td>Sure, let's go\\n\\n### Instruction\\nwhat are 3 types of consciousness\\n\\n### Context\\n\\nBuddhism distinguishes various types of consciousness. These are typically classified into three categories: sensory consciousness (sañña), perception-consciousness (saññā-sañjāna), and consciousness (viññāṇa).\\n\\nSensory consciousness is responsible for registering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### Instruction What are the seven wonders of the world?\\n\\n### Context\\nThe Seven Wonders of the Ancient World, also known as the Seven Wonders of the World or simply the Seven Wonders, is a list of seven notable structures present during classical antiquity. The first known list of seven wonders dates back to the 2nd–1st century BC.\\n\\nWhile the entries have varied over the centuries, the seven traditional wonders are the Great Pyramid of Giza, the Colossus of Rhodes, the Lighthouse of Alexandria, the Mausoleum at Halicarnassus, the Temple of Artemis, the Statue of Zeus at Olympia, and the Hanging Gardens of Babylon. Using modern-day countries, two of the wonders were located in Greece, two in Turkey, two in Egypt, and one in Iraq. Of the seven wonders, only the Pyramid of Giza, which is also by far the oldest of the wonders, still remains standing, with the others being destroyed over the centuries. There is scholarly debate over the exact nature of the Hanging Gardens, and there is doubt as to whether they existed at all.\\n\\n### Response:\\n</td>\n",
       "      <td>The seven wonders of the world consist of; the Great Pyramid of Giza, The Colossus of Rhodes, the Lighthouse of Alexandria, the Mausoleum at Halicarnassus, the Temple of Artemis, the Statue of Zeus at Olympia, and the Hanging Gardens of Babylon.</td>\n",
       "      <td>\\nToday, the Seven Wonders include: The Great Pyramid of Giza in Egypt,  the Statue of Zeus at Olympia, the Aphrodite's Palace in Turkey, the Taj Mahal in India, the Colosseum in Italy, the Eiffel Tower in France, and Machu Picchu in Peru.\\n\\n### Evaluation\\n\\n```\\n2. Write a Java program that will print out the names</td>\n",
       "      <td>\\nThe seven wonders of the world are: the Great Pyramid of Giza (Egypt), the Colossus of Rhodes, the Lighthouse of Alexandria, the Mausoleum at Halicarnassus (modern Turkey), the Temple of Artemis, the Statue of Zeus at Olympia, and the Hanging Gardens of Babylon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### Instruction What are the causes for Sensory processing disorder?\\n\\n### Context\\nThe exact cause of SPD is not known.However, it is known that the midbrain and brainstem regions of the central nervous system are early centers in the processing pathway for multisensory integration; these brain regions are involved in processes including coordination, attention, arousal, and autonomic function. After sensory information passes through these centers, it is then routed to brain regions responsible for emotions, memory, and higher level cognitive functions. Damage in any part of the brain involved in multisensory processing can cause difficulties in adequately processing stimuli in a functional way.\\n\\n### Response:\\n</td>\n",
       "      <td>The exact cause of Sensory processing disorder is not known. However, it is known that the midbrain and brainstem regions of the central nervous system are early centers in the processing pathway for multisensory integration; these brain regions are involved in processes including coordination, attention, arousal, and autonomic function. After sensory information passes through these centers, it is then routed to brain regions responsible for emotions, memory, and higher level cognitive functions. Damage in any part of the brain involved in multisensory processing can cause difficulties in adequately processing stimuli in a functional way.</td>\n",
       "      <td>\\n* [More info...](https://en.wikipedia.org/wiki/Sensory_processing_disorder).\\n* [Included in the topic?](yes, see the first [code example](codes/s0605.md))\\n* [Categories](categories): [06: SPD](categories/06--spd.md).\\n</td>\n",
       "      <td>Some of the known causes of sensory processing disorder are listed below\\n\\n&lt;br&gt;-\\n-\\n\\n### Instruction How is it managed and treated?\\n\\n### Context\\nManaging sensory processing difficulties can involve many different approaches depending on the setting (e.g., home, school, therapeutic office), on the needs of the person, and on the age at which the individual first experiences sensory issues. However, in general, management strateg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### Instruction Does \"outbreeding\" or \"inbreeding\" benefit the offspring more?\\n\\n### Context\\nExogamy often results in two individuals that are not closely genetically related marrying each other; that is, outbreeding as opposed to inbreeding. In moderation, this benefits the offspring as it reduces the risk of the offspring inheriting two copies of a defective gene. Increasing the genetic diversity of the offspring improves the chances of offspring reproducing, up until the fourth-cousin level of relatedness; however, reproduction between individuals on the fourth-cousin level of relatedness decreases evolutionarily fitness.\\n\\n### Response:\\n</td>\n",
       "      <td>\"Outbreeding\" is more beneficial to the offspring as it reduces the risk of inheriting defective genes, increasing the offspring's ability to reproduce.</td>\n",
       "      <td>Inbreeding and outbreeding both benefit the health of the offspring and of the entire species. Outbreeding may reduce the health concerns associated with close blood relatedness, but it also increases the risk of a reduced number of offspring, as individuals of differing bloodlines may have incompatible body chemistry. Inbreeding can also lead to negative phenotypic traits. For example, if one line of rabbits inbreed, their future generations may develop und</td>\n",
       "      <td>Inbreeding/Outbreeding might be better in certain instances as the children might benefit from the increased genetic diversity.\\n\\n### Instruction Expert:\\nOutbreeding helps to lower the occurrence of genetic problems that can be caused by inbreeding. With outbreeding, gene variation can help offset the genetic illnesses that might affect an individual, while inbreeding may have a higher risk of certain genetic anomalies.\\n\\n###</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### Instruction What was the Underground Railroad?\\n\\n### Context\\nThe Underground Railroad was a network of clandestine routes and safe houses established in the United States during the early- to the mid-19th century. It was used by enslaved African Americans primarily to escape into free states and Canada. The network was assisted by abolitionists and others sympathetic to the cause of the escapees. The enslaved persons who risked escape and those who aided them are also collectively referred to as the \"Underground Railroad\". Various other routes led to Mexico, where slavery had been abolished, and to islands in the Caribbean that were not part of the slave trade. An earlier escape route running south toward Florida, then a Spanish possession (except 1763–1783), existed from the late 17th century until approximately 1790. However, the network now generally known as the Underground Railroad began in the late 18th century. It ran north and grew steadily until the Emancipation Proclamation was signed by President Abraham Lincoln. One estimate suggests that by 1850, approximately 100,000 enslaved people had escaped to freedom via the network\\n\\n### Response:\\n</td>\n",
       "      <td>The Underground Railroad was a secret network of routes and safe houses in the United States established in the early 19th century that led to free states, Canada, Mexico and other overseas areas. Black slaves used the Underground Railroad to escape from slavery. It is estimated that by 1850, about 100,000 slaves had escaped to freedom by the \"Railroad\".</td>\n",
       "      <td>\\n* **Did the Underground Railroad really exist?**\\n* **Did the Underground Railroad ever run south**?\\n* **How much help did the railroad give to freedom seekers along the way?**\\n\\n### Evidence\\nAbby, \"The Underground Railroad\" [link]\\nDavis, \"The Underground Railroad\" [link]\\n\\n### Conclusion\\n\\nA network of safe houses along with the c</td>\n",
       "      <td>The Underground Railroad was a way to escape slavery and the people who aided in escaping were called conductors or engineers who hid the fugitive slaves for short distances until they were far enough north where slavery is illegal.\\n\\n\\n### Instruction What did the Underground Railroad use to transport people?\\n\\n### Context\\nThe network of routes and safe houses across the United States was known as the Underground Railroad. It was used by ensla</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    instruction = f\"### Instruction {datapoint['instruction']}\"\n",
    "    context = f\"### Context\\n{datapoint['context']}\" \n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context] if i is not None])\n",
    "    \n",
    "    payload = {\n",
    "        #\"inputs\": template[\"prompt\"].format(\n",
    "        \"inputs\": prompt.format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generation\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generation\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-2-7b-2024-01-01-10-12-26-073\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-2-7b-2024-01-01-10-12-26-069\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-2-7b-2024-01-01-10-12-26-069\n"
     ]
    }
   ],
   "source": [
    "# Delete resources\n",
    "# pretrained_predictor.delete_model()\n",
    "# pretrained_predictor.delete_endpoint()\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What to do next\n",
    "- Understand the various param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35a3af-c45d-4f27-bbc6-af2c3e8bd49f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Appendix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- epoch: The number of passes that the fine-tuning algorithm takes through the training dataset. Must be an integer greater than 1. Default: 5\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 1e-4.\n",
    "- instruction_tuned: Whether to instruction-train the model or not. Must be 'True' or 'False'. Default: 'False'\n",
    "- per_device_train_batch_size: The batch size per GPU core/CPU for training. Must be a positive integer. Default: 4.\n",
    "- per_device_eval_batch_size: The batch size per GPU core/CPU for evaluation. Must be a positive integer. Default: 1\n",
    "- max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this value. Value -1 means using all of training samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this value. Value -1 means using all of validation samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. If -1, max_input_length is set to the minimum of 1024 and the maximum model length defined by the tokenizer. If set to a positive value, max_input_length is set to the minimum of the provided value and the model_max_length defined by the tokenizer. Must be a positive integer or -1. Default: -1. \n",
    "- validation_split_ratio: If validation channel is none, ratio of train-validation split from the train data. Must be between 0 and 1. Default: 0.2. \n",
    "- train_data_split_seed: If validation data is not present, this fixes the random splitting of the input training data to training and validation data used by the algorithm. Must be an integer. Default: 0.\n",
    "- preprocessing_num_workers: The number of processes to use for the preprocessing. If None, main process is used for preprocessing. Default: \"None\"\n",
    "- lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "- lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "- lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "- int8_quantization: If True, model is loaded with 8 bit precision for training. Default for 7B/13B: False. Default for 70B: True.\n",
    "- enable_fsdp: If True, training uses Fully Sharded Data Parallelism. Default for 7B/13B: True. Default for 70B: False.\n",
    "\n",
    "Note 1: int8_quantization is not supported with FSDP. Also, int8_quantization = 'False' and enable_fsdp = 'False' is not supported due to CUDA memory issues for any of the g5 family instances. Thus, we recommend setting exactly one of int8_quantization or enable_fsdp to be 'True'\n",
    "Note 2: Due to the size of the model, 70B model can not be fine-tuned with enable_fsdp = 'True' for any of the supported instance types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6d023-3487-4571-8b52-f332790c1ad7",
   "metadata": {},
   "source": [
    "### 4. Supported Instance types\n",
    "\n",
    "---\n",
    "We have tested our scripts on the following instances types:\n",
    "\n",
    "- 7B: ml.g5.12xlarge, nl.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 13B: ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 70B: ml.g5.48xlarge\n",
    "\n",
    "Other instance types may also work to fine-tune. Note: When using p3 instances, training will be done with 32 bit precision as bfloat16 is not supported on these instances. Thus, training job would consume double the amount of CUDA memory when training on p3 instances compared to g5 instances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d4350-cf4d-40a0-be1c-eba44efd33ab",
   "metadata": {},
   "source": [
    "### Few notes about the fine-tuning method\n",
    "\n",
    "---\n",
    "- Fine-tuning scripts are based on [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). \n",
    "- Instruction tuning dataset is first converted into domain adaptation dataset format before fine-tuning. \n",
    "- Fine-tuning scripts utilize Fully Sharded Data Parallel (FSDP) as well as Low Rank Adaptation (LoRA) method fine-tuning the models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ce841-3a2c-4c08-a102-b94148036a5a",
   "metadata": {},
   "source": [
    "### Studio Kernel Dead/Creating JumpStart Model from the training Job\n",
    "---\n",
    "Due to the size of the Llama 70B model, training job may take several hours and the studio kernel may die during the training phase. However, during this time, training is still running in SageMaker. If this happens, you can still deploy the endpoint using the training job name with the following code:\n",
    "\n",
    "How to find the training job name? Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa60a66-1c2f-42df-8079-191319e28a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "# attached_estimator.logs()\n",
    "# attached_estimator.deploy()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
