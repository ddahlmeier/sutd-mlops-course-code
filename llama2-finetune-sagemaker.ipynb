{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591f0ef4-12a3-441c-a483-f378d2245038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets[s3] in /opt/conda/lib/python3.10/site-packages (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (1.26.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets[s3]) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (0.19.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]) (6.0.1)\n",
      "Collecting s3fs (from datasets[s3])\n",
      "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[s3]) (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets[s3]) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets[s3]) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[s3]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[s3]) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[s3]) (2023.3)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.10/site-packages (from s3fs->datasets[s3]) (2.7.0)\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading s3fs-2023.12.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.9.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.5.4 (from s3fs->datasets[s3])\n",
      "  Downloading aiobotocore-2.5.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting s3fs (from datasets[s3])\n",
      "  Downloading s3fs-2023.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting botocore<1.31.18,>=1.31.17 (from aiobotocore~=2.5.4->s3fs->datasets[s3])\n",
      "  Downloading botocore-1.31.17-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.5.4->s3fs->datasets[s3]) (1.15.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.5.4->s3fs->datasets[s3]) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets[s3]) (1.16.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.4->s3fs->datasets[s3]) (1.0.1)\n",
      "Downloading s3fs-2023.6.0-py3-none-any.whl (28 kB)\n",
      "Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.64\n",
      "    Uninstalling botocore-1.31.64:\n",
      "      Successfully uninstalled botocore-1.31.64\n",
      "  Attempting uninstall: aiobotocore\n",
      "    Found existing installation: aiobotocore 2.7.0\n",
      "    Uninstalling aiobotocore-2.7.0:\n",
      "      Successfully uninstalled aiobotocore-2.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "amazon-sagemaker-jupyter-scheduler 3.0.4 requires aiobotocore==2.7.*, but you have aiobotocore 2.5.4 which is incompatible.\n",
      "boto3 1.28.64 requires botocore<1.32.0,>=1.31.64, but you have botocore 1.31.17 which is incompatible.\n",
      "sagemaker-jupyterlab-extension-common 0.1.9 requires aiobotocore>=2.7.0, but you have aiobotocore 2.5.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.5.4 botocore-1.31.17 s3fs-2023.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets[s3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0651f568-e89d-47e1-b19b-1667d9f669cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::703877312554:role/service-role/AmazonSageMaker-ExecutionRole-20231228T195978\n",
      "sagemaker bucket: sagemaker-us-east-1-703877312554\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d13980-a269-4ef3-8686-a80b9d708449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d595aa126e43a3a470a8ba7a7c9023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a91eac8322e492aa430c0dbfd77e6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69aedfd8d89e40bd8367f96809756f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e19698e89e4901bdc281fe179b11e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8cb316fafa4d8281e754ad7ac99583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'What are the benefits of building a DIY home build off grid?', 'context': '', 'response': 'Some of the benefits could  be:  no local government permits or governance, no state or local government infrastructure bills, reduction of taxes, self sustainability for water, electricity, & support services, privacy, disconnecting from social media, reducing your monthly and yearly operational expenses, getting in touch with nature, reducing clutter in your life and a refocusing on bare essentials for living.', 'category': 'general_qa'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ee5b19-4177-4a00-b220-e987a7edd713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6409265b-ae08-4d8f-825b-0975b56b0a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What is brackish water and where does it occur?\n",
      "\n",
      "### Answer\n",
      "Brackish water is water which is a mixture of freshwater and seawater, and is often found in bays or estuaries\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecb1606-37ef-4dfd-a952-af690a2e70a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a75e00c287490f91aff0af8089f214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c10beec77a4defa126175c4fcd6fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfe11baa3e646d4ba4449c418f3be41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c0bc994119408c86e5a72f3931aac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#model_id = \"meta-llama/Llama-2-13b-hf\" # sharded weights, gated\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # not gated\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aafdd61d-1654-4f06-9fe6-5f5ae99df24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ca555ada674e1890dadd8650af13a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Write a brief passage or elaborate in favour of or against the use or impact of social media in our day to day life.\n",
      "\n",
      "### Answer\n",
      "Information and communication technology has changed rapidly over the past few years. The pace of change is accelerating rapidly. Even at this stage, after a decade or so there is going to be a drastic change in whatever technology we are using around. Onto this note, along with the rise of communication technology the use of social media have risen to a different level. The main reason why people share information on social media is to reveal valuable and entertaining content to others, to grow and nourish relationships and to get the word about brands and interests they had love to use or support. All these factors have caused social media to evolve to keeping in touch with people around that could make a real impact on society. The domains of social media is being used in many ways that shape education, politics, culture, careers, innovation and many more things around. Therefore, social media has changed the way we live our lives. It has redefined the way we imagine our surroundings. Who could have imagined that community networking sites like Twitter, Instagram, Facebook and many more would become a major platform for brands to find potential customers.There are both positive and negative impacts of social media on society as well as businesses. Social media can impact you both positively and negatively. If you are a content creator, or a startup owner, then social media is a great platform for you to meet your customers and people around. However, for individuals, social media is more like an addiction which may cause discomfort if not addressed properly and may impact their mental health. Social media is a great innovation that has changed the way we communicate and interact with each other. The main benefits of social media may include staying update with current affairs going around, communicating anytime and anywhere from the comfort of the any part of the world, advertising platforms for brands, easy to build relationships and connecting with people around and an easy access to desired information, products and services. However, there are also several disadvantages onto the same like lack of understanding and thoughtfulness which could be a major challenge going ahead in coping up things around, decrease in real face to face conversations , affects social emotional connection and cyberbullying. So, we do need to be responsible while using and the awareness around is quite more important before using it freely.</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c477ec1427ef444fab15620d158ad78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08353268f984c2f870cf0e1e2774d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1591\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77ad8d6c-44e0-437b-b5c9-88ccf39fedfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819144a08e4c46fcb3523e778f837d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1591 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-703877312554/processed/llama/dolly/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "509879b0-c785-4618-a82d-3c94f7d5824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 3,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9cfd9c-e908-41da-bf82-cfa5d3005c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2023-12-31-15-23-48-2023-12-31-15-23-49-019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-12-31 15:23:49 Starting - Starting the training job...\n",
      "2023-12-31 15:24:05 Starting - Preparing the instances for training......\n",
      "2023-12-31 15:25:13 Downloading - Downloading input data...\n",
      "2023-12-31 15:25:30 Downloading - Downloading the training image...............\n",
      "2023-12-31 15:28:21 Training - Training image download completed. Training in progress........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:17,181 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:17,205 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:17,213 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:17,215 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:18,547 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.31.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 96.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft==0.4.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 25.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.21.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 56.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.40.2 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.5/92.5 MB 30.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 113.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, safetensors, transformers, accelerate, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.2 peft-0.4.0 safetensors-0.4.1 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:29,110 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:29,110 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:29,160 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:29,194 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:29,228 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:29,239 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 3,\n",
      "        \"lr\": 0.0002,\n",
      "        \"merge_weights\": true,\n",
      "        \"model_id\": \"NousResearch/Llama-2-7b-hf\",\n",
      "        \"per_device_train_batch_size\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-2023-12-31-15-23-48-2023-12-31-15-23-49-019\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-703877312554/huggingface-qlora-2023-12-31-15-23-48-2023-12-31-15-23-49-019/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":3,\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"NousResearch/Llama-2-7b-hf\",\"per_device_train_batch_size\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_clm.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_clm\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-703877312554/huggingface-qlora-2023-12-31-15-23-48-2023-12-31-15-23-49-019/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":3,\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"NousResearch/Llama-2-7b-hf\",\"per_device_train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-2023-12-31-15-23-48-2023-12-31-15-23-49-019\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-703877312554/huggingface-qlora-2023-12-31-15-23-48-2023-12-31-15-23-49-019/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"3\",\"--lr\",\"0.0002\",\"--merge_weights\",\"True\",\"--model_id\",\"NousResearch/Llama-2-7b-hf\",\"--per_device_train_batch_size\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_WEIGHTS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=NousResearch/Llama-2-7b-hf\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --epochs 3 --lr 0.0002 --merge_weights True --model_id NousResearch/Llama-2-7b-hf --per_device_train_batch_size 2\u001b[0m\n",
      "\u001b[34m2023-12-31 15:29:29,265 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mDownloading config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading config.json: 100%|██████████| 583/583 [00:00<00:00, 6.74MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 172MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 62.9M/9.98G [00:00<00:17, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|▏         | 126M/9.98G [00:00<00:17, 573MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 189M/9.98G [00:00<00:16, 581MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 252M/9.98G [00:00<00:17, 556MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 315M/9.98G [00:00<00:18, 536MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 377M/9.98G [00:00<00:17, 551MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 440M/9.98G [00:00<00:17, 556MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▌         | 503M/9.98G [00:00<00:17, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 566M/9.98G [00:01<00:17, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▋         | 629M/9.98G [00:01<00:18, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 682M/9.98G [00:01<00:18, 512MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 734M/9.98G [00:01<00:18, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 797M/9.98G [00:01<00:17, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▊         | 860M/9.98G [00:01<00:17, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 912M/9.98G [00:01<00:17, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|▉         | 975M/9.98G [00:01<00:16, 537MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|█         | 1.04G/9.98G [00:01<00:16, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 1.10G/9.98G [00:02<00:16, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.16G/9.98G [00:02<00:16, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.23G/9.98G [00:02<00:16, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.29G/9.98G [00:02<00:16, 521MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.34G/9.98G [00:02<00:16, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 1.41G/9.98G [00:02<00:16, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 1.47G/9.98G [00:02<00:15, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▌        | 1.53G/9.98G [00:02<00:15, 554MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 1.59G/9.98G [00:02<00:14, 562MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.66G/9.98G [00:03<00:15, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.72G/9.98G [00:03<00:15, 550MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.78G/9.98G [00:03<00:14, 556MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.85G/9.98G [00:03<00:16, 503MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 1.91G/9.98G [00:03<00:15, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|█▉        | 1.97G/9.98G [00:03<00:15, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|██        | 2.03G/9.98G [00:03<00:15, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 2.10G/9.98G [00:03<00:14, 534MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.16G/9.98G [00:04<00:14, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.22G/9.98G [00:04<00:14, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.29G/9.98G [00:04<00:13, 552MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▎       | 2.35G/9.98G [00:04<00:14, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 2.41G/9.98G [00:04<00:14, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▍       | 2.47G/9.98G [00:04<00:13, 545MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 2.54G/9.98G [00:04<00:13, 555MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 2.60G/9.98G [00:04<00:13, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.66G/9.98G [00:04<00:13, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.73G/9.98G [00:05<00:13, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.79G/9.98G [00:05<00:13, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▊       | 2.85G/9.98G [00:05<00:13, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 2.92G/9.98G [00:05<00:12, 554MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|██▉       | 2.98G/9.98G [00:05<00:12, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|███       | 3.04G/9.98G [00:05<00:13, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 3.10G/9.98G [00:05<00:13, 518MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.17G/9.98G [00:05<00:13, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.23G/9.98G [00:06<00:13, 498MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.29G/9.98G [00:06<00:12, 519MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▎      | 3.36G/9.98G [00:06<00:12, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 3.42G/9.98G [00:06<00:12, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▍      | 3.47G/9.98G [00:06<00:12, 501MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▌      | 3.53G/9.98G [00:06<00:12, 511MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 3.60G/9.98G [00:06<00:12, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.65G/9.98G [00:06<00:12, 510MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.71G/9.98G [00:06<00:11, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.77G/9.98G [00:07<00:11, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.84G/9.98G [00:07<00:11, 536MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 3.90G/9.98G [00:07<00:11, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|███▉      | 3.96G/9.98G [00:07<00:11, 533MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|████      | 4.03G/9.98G [00:07<00:10, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 4.09G/9.98G [00:07<00:10, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.15G/9.98G [00:07<00:10, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.22G/9.98G [00:07<00:10, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.28G/9.98G [00:08<00:10, 537MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▎     | 4.34G/9.98G [00:08<00:10, 549MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 4.40G/9.98G [00:08<00:09, 558MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▍     | 4.47G/9.98G [00:08<00:09, 566MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▌     | 4.53G/9.98G [00:08<00:10, 522MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 4.59G/9.98G [00:08<00:09, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.66G/9.98G [00:08<00:09, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.72G/9.98G [00:08<00:09, 557MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.78G/9.98G [00:08<00:09, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▊     | 4.84G/9.98G [00:09<00:09, 555MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 4.91G/9.98G [00:09<00:09, 562MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|████▉     | 4.97G/9.98G [00:09<00:08, 566MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 5.03G/9.98G [00:09<00:08, 552MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████     | 5.10G/9.98G [00:09<00:08, 562MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.16G/9.98G [00:09<00:08, 561MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.22G/9.98G [00:09<00:08, 566MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.28G/9.98G [00:09<00:08, 537MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▎    | 5.35G/9.98G [00:09<00:08, 551MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 5.41G/9.98G [00:10<00:08, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▍    | 5.47G/9.98G [00:10<00:08, 526MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▌    | 5.54G/9.98G [00:10<00:08, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 5.60G/9.98G [00:10<00:07, 557MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.66G/9.98G [00:10<00:07, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.73G/9.98G [00:10<00:11, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.78G/9.98G [00:10<00:10, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▊    | 5.84G/9.98G [00:11<00:09, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.90G/9.98G [00:11<00:08, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|█████▉    | 5.97G/9.98G [00:11<00:07, 503MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|██████    | 6.03G/9.98G [00:11<00:07, 506MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 6.09G/9.98G [00:11<00:07, 497MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.14G/9.98G [00:11<00:07, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.21G/9.98G [00:11<00:07, 522MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.27G/9.98G [00:11<00:06, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.33G/9.98G [00:11<00:06, 554MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 6.40G/9.98G [00:12<00:06, 560MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▍   | 6.46G/9.98G [00:12<00:06, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 6.52G/9.98G [00:12<00:06, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 6.59G/9.98G [00:12<00:06, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.65G/9.98G [00:12<00:06, 552MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.71G/9.98G [00:12<00:06, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 6.77G/9.98G [00:12<00:05, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▊   | 6.84G/9.98G [00:12<00:05, 537MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▉   | 6.90G/9.98G [00:13<00:05, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 6.96G/9.98G [00:13<00:07, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|███████   | 7.01G/9.98G [00:13<00:07, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 7.07G/9.98G [00:13<00:06, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████▏  | 7.12G/9.98G [00:13<00:07, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.17G/9.98G [00:13<00:06, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.22G/9.98G [00:13<00:06, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.28G/9.98G [00:13<00:06, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.33G/9.98G [00:14<00:05, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 7.38G/9.98G [00:14<00:05, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▍  | 7.44G/9.98G [00:14<00:05, 480MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 7.51G/9.98G [00:14<00:04, 496MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 7.56G/9.98G [00:14<00:04, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▋  | 7.61G/9.98G [00:14<00:04, 494MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.67G/9.98G [00:14<00:04, 497MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.72G/9.98G [00:14<00:04, 492MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.78G/9.98G [00:14<00:04, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▊  | 7.84G/9.98G [00:15<00:04, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 7.91G/9.98G [00:15<00:03, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|███████▉  | 7.97G/9.98G [00:15<00:03, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.03G/9.98G [00:15<00:03, 550MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.10G/9.98G [00:15<00:03, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.16G/9.98G [00:15<00:03, 536MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.22G/9.98G [00:15<00:03, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.28G/9.98G [00:15<00:03, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▎ | 8.35G/9.98G [00:16<00:03, 534MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.41G/9.98G [00:16<00:02, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▍ | 8.47G/9.98G [00:16<00:02, 539MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 8.54G/9.98G [00:16<00:02, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 8.60G/9.98G [00:16<00:02, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.66G/9.98G [00:16<00:02, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.72G/9.98G [00:16<00:02, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.79G/9.98G [00:16<00:02, 548MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▊ | 8.85G/9.98G [00:16<00:02, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 8.91G/9.98G [00:17<00:01, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 8.98G/9.98G [00:17<00:01, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.04G/9.98G [00:17<00:01, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.10G/9.98G [00:17<00:01, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.16G/9.98G [00:17<00:01, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.23G/9.98G [00:17<00:01, 553MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.29G/9.98G [00:17<00:01, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 9.35G/9.98G [00:17<00:01, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 9.42G/9.98G [00:18<00:01, 532MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▌| 9.48G/9.98G [00:18<00:00, 536MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.54G/9.98G [00:18<00:00, 521MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.59G/9.98G [00:18<00:00, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.65G/9.98G [00:18<00:00, 515MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.71G/9.98G [00:18<00:00, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.77G/9.98G [00:18<00:00, 546MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▊| 9.84G/9.98G [00:18<00:00, 555MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 9.90G/9.98G [00:18<00:00, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|█████████▉| 9.96G/9.98G [00:19<00:00, 556MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:19<00:00, 523MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:19<00:19, 19.10s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 62.9M/3.50G [00:00<00:05, 578MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▎         | 126M/3.50G [00:00<00:06, 515MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▌         | 189M/3.50G [00:00<00:06, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 252M/3.50G [00:00<00:05, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 315M/3.50G [00:00<00:06, 519MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 377M/3.50G [00:00<00:06, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 440M/3.50G [00:00<00:05, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 503M/3.50G [00:00<00:05, 549MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 566M/3.50G [00:01<00:05, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 629M/3.50G [00:01<00:05, 534MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|█▉        | 692M/3.50G [00:01<00:05, 527MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 755M/3.50G [00:01<00:05, 542MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 818M/3.50G [00:01<00:04, 543MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 881M/3.50G [00:01<00:04, 534MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 944M/3.50G [00:01<00:04, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 1.01G/3.50G [00:01<00:04, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 1.07G/3.50G [00:01<00:04, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 1.13G/3.50G [00:02<00:04, 531MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 1.20G/3.50G [00:02<00:04, 509MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 1.26G/3.50G [00:02<00:04, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 1.32G/3.50G [00:02<00:04, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|███▉      | 1.38G/3.50G [00:02<00:03, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████▏     | 1.45G/3.50G [00:02<00:03, 541MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 1.51G/3.50G [00:02<00:03, 533MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▍     | 1.57G/3.50G [00:02<00:03, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▋     | 1.63G/3.50G [00:03<00:03, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 1.69G/3.50G [00:03<00:03, 528MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 1.75G/3.50G [00:03<00:03, 499MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 1.81G/3.50G [00:03<00:03, 512MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▎    | 1.88G/3.50G [00:03<00:03, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▌    | 1.94G/3.50G [00:03<00:02, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 2.00G/3.50G [00:03<00:02, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 2.07G/3.50G [00:03<00:02, 524MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 2.13G/3.50G [00:04<00:02, 540MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 2.19G/3.50G [00:04<00:02, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 2.25G/3.50G [00:04<00:02, 548MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 2.32G/3.50G [00:04<00:02, 559MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 2.38G/3.50G [00:04<00:01, 566MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 2.44G/3.50G [00:04<00:01, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 2.51G/3.50G [00:04<00:01, 527MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 2.57G/3.50G [00:04<00:01, 544MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 2.63G/3.50G [00:04<00:01, 551MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 2.69G/3.50G [00:05<00:01, 562MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 2.76G/3.50G [00:05<00:01, 516MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 2.82G/3.50G [00:05<00:01, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 2.88G/3.50G [00:05<00:01, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 2.95G/3.50G [00:05<00:01, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 3.01G/3.50G [00:05<00:00, 538MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 3.07G/3.50G [00:05<00:00, 547MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 3.14G/3.50G [00:05<00:00, 535MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████▏| 3.20G/3.50G [00:06<00:00, 517MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 3.26G/3.50G [00:06<00:00, 530MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▍| 3.32G/3.50G [00:06<00:00, 523MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 3.39G/3.50G [00:06<00:00, 501MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▊| 3.45G/3.50G [00:06<00:00, 525MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:06<00:00, 531MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 11.77s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.87s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\u001b[0m\n",
      "\u001b[34mDownloading generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading generation_config.json: 100%|██████████| 179/179 [00:00<00:00, 2.21MB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['gate_proj', 'k_proj', 'q_proj', 'up_proj', 'o_proj', 'down_proj', 'v_proj']\u001b[0m\n",
      "\u001b[34mtrainable params: 159,907,840 || all params: 3,660,320,768 || trainable%: 4.368683788535114\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/2388 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/2388 [00:08<5:22:54,  8.12s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/2388 [00:16<5:18:59,  8.02s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/2388 [00:24<5:17:37,  7.99s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/2388 [00:31<5:16:55,  7.98s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/2388 [00:39<5:16:28,  7.97s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/2388 [00:47<5:16:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 7/2388 [00:55<5:15:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 8/2388 [01:03<5:15:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 9/2388 [01:11<5:15:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 10/2388 [01:19<5:15:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6599, 'learning_rate': 0.00019916247906197655, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m0%|          | 10/2388 [01:19<5:15:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 11/2388 [01:27<5:15:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 12/2388 [01:35<5:15:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 13/2388 [01:43<5:14:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 14/2388 [01:51<5:14:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 15/2388 [01:59<5:14:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 16/2388 [02:07<5:14:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 17/2388 [02:15<5:14:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 18/2388 [02:23<5:14:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 19/2388 [02:31<5:14:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 20/2388 [02:39<5:13:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5599, 'learning_rate': 0.0001983249581239531, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m1%|          | 20/2388 [02:39<5:13:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 21/2388 [02:47<5:13:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 22/2388 [02:55<5:13:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 23/2388 [03:03<5:13:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 24/2388 [03:11<5:13:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 25/2388 [03:19<5:13:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 26/2388 [03:27<5:13:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 27/2388 [03:34<5:13:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 28/2388 [03:42<5:12:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 29/2388 [03:50<5:12:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 30/2388 [03:58<5:12:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.499, 'learning_rate': 0.00019748743718592964, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m1%|▏         | 30/2388 [03:58<5:12:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 31/2388 [04:06<5:12:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 32/2388 [04:14<5:12:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 33/2388 [04:22<5:12:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 34/2388 [04:30<5:12:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 35/2388 [04:38<5:11:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 36/2388 [04:46<5:11:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 37/2388 [04:54<5:11:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 38/2388 [05:02<5:11:35,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 39/2388 [05:10<5:11:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 40/2388 [05:18<5:11:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4491, 'learning_rate': 0.0001966499162479062, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m2%|▏         | 40/2388 [05:18<5:11:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 41/2388 [05:26<5:11:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 42/2388 [05:34<5:11:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 43/2388 [05:42<5:10:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 44/2388 [05:50<5:10:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 45/2388 [05:58<5:10:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 46/2388 [06:06<5:10:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 47/2388 [06:14<5:10:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 48/2388 [06:22<5:10:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 49/2388 [06:29<5:10:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 50/2388 [06:37<5:10:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4625, 'learning_rate': 0.00019581239530988274, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m2%|▏         | 50/2388 [06:37<5:10:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 51/2388 [06:45<5:09:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 52/2388 [06:53<5:09:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 53/2388 [07:01<5:09:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 54/2388 [07:09<5:09:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 55/2388 [07:17<5:09:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 56/2388 [07:25<5:09:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 57/2388 [07:33<5:09:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 58/2388 [07:41<5:09:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 59/2388 [07:49<5:08:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 60/2388 [07:57<5:08:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3793, 'learning_rate': 0.0001949748743718593, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m3%|▎         | 60/2388 [07:57<5:08:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 61/2388 [08:05<5:08:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 62/2388 [08:13<5:08:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 63/2388 [08:21<5:08:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 64/2388 [08:29<5:08:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 65/2388 [08:37<5:08:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 66/2388 [08:45<5:07:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 67/2388 [08:53<5:07:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 68/2388 [09:01<5:07:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 69/2388 [09:09<5:07:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 70/2388 [09:17<5:07:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4121, 'learning_rate': 0.00019413735343383584, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m3%|▎         | 70/2388 [09:17<5:07:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 71/2388 [09:25<5:07:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 72/2388 [09:32<5:07:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 73/2388 [09:40<5:06:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 74/2388 [09:48<5:06:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 75/2388 [09:56<5:06:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 76/2388 [10:04<5:06:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 77/2388 [10:12<5:06:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 78/2388 [10:20<5:06:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 79/2388 [10:28<5:06:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 80/2388 [10:36<5:06:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4338, 'learning_rate': 0.0001932998324958124, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m3%|▎         | 80/2388 [10:36<5:06:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 81/2388 [10:44<5:05:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 82/2388 [10:52<5:05:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 83/2388 [11:00<5:05:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 84/2388 [11:08<5:05:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 85/2388 [11:16<5:05:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 86/2388 [11:24<5:05:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 87/2388 [11:32<5:05:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 88/2388 [11:40<5:04:55,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 89/2388 [11:48<5:04:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 90/2388 [11:56<5:04:40,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4203, 'learning_rate': 0.00019246231155778894, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m4%|▍         | 90/2388 [11:56<5:04:40,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 91/2388 [12:04<5:04:32,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 92/2388 [12:12<5:04:24,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 93/2388 [12:20<5:04:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 94/2388 [12:27<5:04:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 95/2388 [12:35<5:04:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 96/2388 [12:43<5:03:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 97/2388 [12:51<5:03:44,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 98/2388 [12:59<5:03:36,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 99/2388 [13:07<5:03:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 100/2388 [13:15<5:03:20,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3743, 'learning_rate': 0.0001916247906197655, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m4%|▍         | 100/2388 [13:15<5:03:20,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 101/2388 [13:23<5:03:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 102/2388 [13:31<5:03:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 103/2388 [13:39<5:02:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 104/2388 [13:47<5:02:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 105/2388 [13:55<5:02:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 106/2388 [14:03<5:02:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 107/2388 [14:11<5:02:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 108/2388 [14:19<5:02:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 109/2388 [14:27<5:02:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 110/2388 [14:35<5:02:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3352, 'learning_rate': 0.00019078726968174204, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m5%|▍         | 110/2388 [14:35<5:02:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 111/2388 [14:43<5:01:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 112/2388 [14:51<5:01:47,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 113/2388 [14:59<5:01:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 114/2388 [15:07<5:01:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 115/2388 [15:15<5:01:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 116/2388 [15:23<5:01:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 117/2388 [15:30<5:01:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 118/2388 [15:38<5:00:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 119/2388 [15:46<5:00:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 120/2388 [15:54<5:00:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4775, 'learning_rate': 0.0001899497487437186, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m5%|▌         | 120/2388 [15:54<5:00:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 121/2388 [16:02<5:00:35,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 122/2388 [16:10<5:00:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 123/2388 [16:18<5:00:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 124/2388 [16:26<5:00:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 125/2388 [16:34<5:00:03,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 126/2388 [16:42<4:59:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 127/2388 [16:50<4:59:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 128/2388 [16:58<4:59:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 129/2388 [17:06<4:59:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 130/2388 [17:14<4:59:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3623, 'learning_rate': 0.00018911222780569514, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m5%|▌         | 130/2388 [17:14<4:59:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 131/2388 [17:22<4:59:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 132/2388 [17:30<4:59:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 133/2388 [17:38<4:59:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 134/2388 [17:46<4:58:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 135/2388 [17:54<4:58:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 136/2388 [18:02<4:58:35,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 137/2388 [18:10<4:58:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 138/2388 [18:18<4:58:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 139/2388 [18:25<4:58:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 140/2388 [18:33<4:58:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3973, 'learning_rate': 0.0001882747068676717, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m6%|▌         | 140/2388 [18:33<4:58:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 141/2388 [18:41<4:57:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 142/2388 [18:49<4:57:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 143/2388 [18:57<4:57:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 144/2388 [19:05<4:57:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 145/2388 [19:13<4:57:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 146/2388 [19:21<4:57:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 147/2388 [19:29<4:57:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 148/2388 [19:37<4:57:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 149/2388 [19:45<4:56:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 150/2388 [19:53<4:56:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3477, 'learning_rate': 0.00018743718592964824, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m6%|▋         | 150/2388 [19:53<4:56:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 151/2388 [20:01<4:56:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 152/2388 [20:09<4:56:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 153/2388 [20:17<4:56:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 154/2388 [20:25<4:56:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 155/2388 [20:33<4:56:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 156/2388 [20:41<4:55:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 157/2388 [20:49<4:55:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 158/2388 [20:57<4:55:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 159/2388 [21:05<4:55:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 160/2388 [21:13<4:55:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3911, 'learning_rate': 0.0001865996649916248, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m7%|▋         | 160/2388 [21:13<4:55:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 161/2388 [21:21<4:55:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 162/2388 [21:28<4:55:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 163/2388 [21:36<4:55:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 164/2388 [21:44<4:54:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 165/2388 [21:52<4:54:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 166/2388 [22:00<4:54:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 167/2388 [22:08<4:54:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 168/2388 [22:16<4:54:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 169/2388 [22:24<4:54:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 170/2388 [22:32<4:54:03,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4427, 'learning_rate': 0.00018576214405360134, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m7%|▋         | 170/2388 [22:32<4:54:03,  7.95s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 171/2388 [22:40<4:53:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 172/2388 [22:48<4:53:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 173/2388 [22:56<4:53:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 174/2388 [23:04<4:53:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 175/2388 [23:12<4:53:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 176/2388 [23:20<4:53:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 177/2388 [23:28<4:53:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 178/2388 [23:36<4:53:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 179/2388 [23:44<4:52:52,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 180/2388 [23:52<4:52:43,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4569, 'learning_rate': 0.0001849246231155779, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m8%|▊         | 180/2388 [23:52<4:52:43,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 181/2388 [24:00<4:52:36,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 182/2388 [24:08<4:52:28,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 183/2388 [24:16<4:52:20,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 184/2388 [24:23<4:52:12,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 185/2388 [24:31<4:52:04,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 186/2388 [24:39<4:51:56,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 187/2388 [24:47<4:51:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 188/2388 [24:55<4:51:40,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 189/2388 [25:03<4:51:32,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 190/2388 [25:11<4:51:24,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3543, 'learning_rate': 0.00018408710217755444, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m8%|▊         | 190/2388 [25:11<4:51:24,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 191/2388 [25:19<4:51:16,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 192/2388 [25:27<4:51:08,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 193/2388 [25:35<4:51:01,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 194/2388 [25:43<4:50:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 195/2388 [25:51<4:50:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 196/2388 [25:59<4:50:36,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 197/2388 [26:07<4:50:28,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 198/2388 [26:15<4:50:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 199/2388 [26:23<4:50:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 200/2388 [26:31<4:50:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4831, 'learning_rate': 0.000183249581239531, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m8%|▊         | 200/2388 [26:31<4:50:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 201/2388 [26:39<4:49:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 202/2388 [26:47<4:49:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 203/2388 [26:55<4:49:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 204/2388 [27:03<4:49:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 205/2388 [27:11<4:49:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 206/2388 [27:18<4:49:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 207/2388 [27:26<4:49:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 208/2388 [27:34<4:49:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 209/2388 [27:42<4:48:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 210/2388 [27:50<4:48:47,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3855, 'learning_rate': 0.00018241206030150754, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m9%|▉         | 210/2388 [27:50<4:48:47,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 211/2388 [27:58<4:48:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 212/2388 [28:06<4:48:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 213/2388 [28:14<4:48:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 214/2388 [28:22<4:48:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 215/2388 [28:30<4:48:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 216/2388 [28:38<4:47:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 217/2388 [28:46<4:47:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 218/2388 [28:54<4:47:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 219/2388 [29:02<4:47:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 220/2388 [29:10<4:47:25,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3451, 'learning_rate': 0.0001815745393634841, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m9%|▉         | 220/2388 [29:10<4:47:25,  7.95s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 221/2388 [29:18<4:47:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 222/2388 [29:26<4:47:10,  7.95s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 223/2388 [29:34<4:47:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 224/2388 [29:42<4:46:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 225/2388 [29:50<4:46:47,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 226/2388 [29:58<4:46:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 227/2388 [30:06<4:46:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 228/2388 [30:14<4:46:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 229/2388 [30:21<4:46:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 230/2388 [30:29<4:46:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3843, 'learning_rate': 0.00018073701842546063, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m10%|▉         | 230/2388 [30:29<4:46:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 231/2388 [30:37<4:46:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 232/2388 [30:45<4:45:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 233/2388 [30:53<4:45:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 234/2388 [31:01<4:45:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 235/2388 [31:09<4:45:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 236/2388 [31:17<4:45:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 237/2388 [31:25<4:45:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 238/2388 [31:33<4:45:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 239/2388 [31:41<4:44:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 240/2388 [31:49<4:44:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4091, 'learning_rate': 0.0001798994974874372, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m10%|█         | 240/2388 [31:49<4:44:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 241/2388 [31:57<4:44:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 242/2388 [32:05<4:44:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 243/2388 [32:13<4:44:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 244/2388 [32:21<4:44:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 245/2388 [32:29<4:44:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 246/2388 [32:37<4:44:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 247/2388 [32:45<4:43:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 248/2388 [32:53<4:43:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 249/2388 [33:01<4:43:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 250/2388 [33:09<4:43:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4691, 'learning_rate': 0.00017906197654941373, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m10%|█         | 250/2388 [33:09<4:43:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 251/2388 [33:16<4:43:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 252/2388 [33:24<4:43:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 253/2388 [33:32<4:43:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 254/2388 [33:40<4:42:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 255/2388 [33:48<4:42:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 256/2388 [33:56<4:42:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 257/2388 [34:04<4:42:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 258/2388 [34:12<4:42:25,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 259/2388 [34:20<4:42:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 284/2388 [37:39<4:38:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 285/2388 [37:47<4:38:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 286/2388 [37:55<4:38:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 287/2388 [38:03<4:38:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 288/2388 [38:11<4:38:25,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 289/2388 [38:19<4:38:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 290/2388 [38:27<4:38:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4377, 'learning_rate': 0.00017571189279731993, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 290/2388 [38:27<4:38:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 291/2388 [38:35<4:38:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 292/2388 [38:43<4:37:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 293/2388 [38:51<4:37:45,  7.95s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 294/2388 [38:59<4:37:37,  7.95s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 295/2388 [39:07<4:37:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 296/2388 [39:14<4:37:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 297/2388 [39:22<4:37:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 298/2388 [39:30<4:37:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 299/2388 [39:38<4:36:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 300/2388 [39:46<4:36:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3944, 'learning_rate': 0.0001748743718592965, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 300/2388 [39:46<4:36:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 301/2388 [39:54<4:36:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 302/2388 [40:02<4:36:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 303/2388 [40:10<4:36:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 304/2388 [40:18<4:36:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 305/2388 [40:26<4:36:10,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 306/2388 [40:34<4:36:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 307/2388 [40:42<4:35:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 308/2388 [40:50<4:35:45,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 309/2388 [40:58<4:35:37,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 310/2388 [41:06<4:35:29,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4546, 'learning_rate': 0.00017403685092127303, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 310/2388 [41:06<4:35:29,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 311/2388 [41:14<4:35:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 312/2388 [41:22<4:35:14,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 313/2388 [41:30<4:35:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 314/2388 [41:38<4:34:57,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 315/2388 [41:46<4:34:49,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 316/2388 [41:54<4:34:41,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 317/2388 [42:02<4:34:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 318/2388 [42:09<4:34:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 319/2388 [42:17<4:34:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 320/2388 [42:25<4:34:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3574, 'learning_rate': 0.0001731993299832496, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 320/2388 [42:25<4:34:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 321/2388 [42:33<4:34:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 322/2388 [42:41<4:33:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 323/2388 [42:49<4:33:47,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 324/2388 [42:57<4:33:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 325/2388 [43:05<4:33:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 326/2388 [43:13<4:33:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 327/2388 [43:21<4:33:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 328/2388 [43:29<4:33:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 329/2388 [43:37<4:32:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 330/2388 [43:45<4:32:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3899, 'learning_rate': 0.00017236180904522613, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 330/2388 [43:45<4:32:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 331/2388 [43:53<4:32:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 332/2388 [44:01<4:32:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 333/2388 [44:09<4:32:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 334/2388 [44:17<4:32:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 335/2388 [44:25<4:32:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 336/2388 [44:33<4:32:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 337/2388 [44:41<4:31:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 338/2388 [44:49<4:31:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 339/2388 [44:57<4:31:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 340/2388 [45:04<4:31:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3825, 'learning_rate': 0.0001715242881072027, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 340/2388 [45:04<4:31:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 341/2388 [45:12<4:31:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 342/2388 [45:20<4:31:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 343/2388 [45:28<4:31:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 344/2388 [45:36<4:31:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 345/2388 [45:44<4:30:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 346/2388 [45:52<4:30:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 347/2388 [46:00<4:30:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 348/2388 [46:08<4:30:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 349/2388 [46:16<4:30:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 350/2388 [46:24<4:30:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4262, 'learning_rate': 0.00017068676716917923, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 350/2388 [46:24<4:30:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 351/2388 [46:32<4:30:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 352/2388 [46:40<4:29:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 353/2388 [46:48<4:29:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 354/2388 [46:56<4:29:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 355/2388 [47:04<4:29:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 356/2388 [47:12<4:29:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 357/2388 [47:20<4:29:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 358/2388 [47:28<4:29:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 359/2388 [47:36<4:29:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 360/2388 [47:44<4:28:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4654, 'learning_rate': 0.0001698492462311558, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 360/2388 [47:44<4:28:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 361/2388 [47:52<4:28:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 362/2388 [48:00<4:28:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 363/2388 [48:07<4:28:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 364/2388 [48:15<4:28:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 365/2388 [48:23<4:28:12,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 366/2388 [48:31<4:28:04,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 367/2388 [48:39<4:27:56,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 368/2388 [48:47<4:27:48,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 369/2388 [48:55<4:27:41,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 370/2388 [49:03<4:27:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4133, 'learning_rate': 0.00016901172529313233, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 370/2388 [49:03<4:27:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 371/2388 [49:11<4:27:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 372/2388 [49:19<4:27:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 373/2388 [49:27<4:27:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 374/2388 [49:35<4:27:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 375/2388 [49:43<4:26:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 376/2388 [49:51<4:26:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 377/2388 [49:59<4:26:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 378/2388 [50:07<4:26:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 379/2388 [50:15<4:26:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 380/2388 [50:23<4:26:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.361, 'learning_rate': 0.0001681742043551089, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 380/2388 [50:23<4:26:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 381/2388 [50:31<4:26:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 382/2388 [50:39<4:25:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 383/2388 [50:47<4:25:49,  7.95s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 384/2388 [50:55<4:25:41,  7.95s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 385/2388 [51:02<4:25:33,  7.96s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b8ec6-44da-42b8-9f46-faf536c31207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
