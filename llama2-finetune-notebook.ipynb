{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b44ac5-a6e5-4deb-bf47-9daf2db61b1a",
   "metadata": {},
   "source": [
    "# Finetune Llama-2 7B in Sagemaker notebook \n",
    "\n",
    "From https://aws.amazon.com/blogs/machine-learning/interactively-fine-tune-falcon-40b-and-other-llms-on-amazon-sagemaker-studio-notebooks-using-qlora/\n",
    "and https://medium.com/@ogbanugot/notes-on-fine-tuning-llama-2-using-qlora-a-detailed-breakdown-370be42ccca1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985305a-85aa-48de-a454-9fe506fc8d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U torch==2.0.1 bitsandbytes==0.40.2\n",
    "%pip install -q -U transformers==4.31.0 peft==0.4.0 accelerate==0.21.0\n",
    "%pip install -q -U datasets py7zr einops tensorboardX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5caf4-a7d9-41d7-9b3e-4131ce0d9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add installed cuda runtime to path for bitsandbytes\n",
    "import os\n",
    "import nvidia\n",
    "\n",
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9ea2e-f7b9-4cf8-bee9-c72aa69645e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # not gated\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "\tload_in_4bit=True,\n",
    "\tbnb_4bit_use_double_quant=True,\n",
    "\tbnb_4bit_quant_type=\"nf4\",\n",
    "\tbnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bf0d8-cb94-4940-9fd9-dbcb2fd931fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860752c7-7838-45a3-8f9e-37d02e1cc2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fcea72-fa96-44fe-b7bb-cfc58c3fb67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea563ea-81ab-431a-835f-83d7614838bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub, load subset of training data for faster training\n",
    "train_dataset = load_dataset(\"samsum\", split=\"train[:10%]\")\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9295d3-3a1a-467e-a2ca-0324241a8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# custom instruct prompt start\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = prompt_template.format(dialogue=sample[\"dialogue\"],\n",
    "                                            summary=sample[\"summary\"],\n",
    "                                            eos_token=tokenizer.eos_token)\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "train_dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "\n",
    "print(train_dataset[randint(0, len(train_dataset))][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce064869-1524-4c01-9501-081849f5926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply prompt template per sample\n",
    "test_dataset = test_dataset.map(template_dataset, remove_columns=list(test_dataset.features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f4954-12a5-4424-a833-f2e43c90656f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # tokenize and chunk dataset\n",
    "lm_train_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, batch_size=24, remove_columns=list(train_dataset.features)\n",
    ")\n",
    "\n",
    "lm_test_dataset = test_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(test_dataset.features)\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of train samples: {len(lm_train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff749dfe-a7f2-4c8a-8f47-acf0f745a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# We set num_train_epochs=1 simply to run a demonstration\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=lm_train_dataset,\n",
    "    eval_dataset=lm_test_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        bf16=True,\n",
    "        output_dir=\"outputs\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inferenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f80bc-d3d5-4551-b152-c93b9a95a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e719598-be19-4f18-ac76-ec56e670a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate and return the metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda65b6-85e6-4fb0-bf73-9f2e764dd91c",
   "metadata": {},
   "source": [
    "Load test dataset and try a random sample for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39937ab-d58b-46bd-a742-f7798e05b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")\n",
    "\n",
    "# select a random test sample\n",
    "sample = test_dataset[randint(0, len(test_dataset))]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_sample = prompt_template.format(dialogue=sample[\"dialogue\"])\n",
    "\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7443a2b-d0a6-41bf-8f75-608df81b6b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(test_sample, return_tensors=\"pt\").input_ids\n",
    "\n",
    "#set the token length for the summary evaluation\n",
    "tokens_for_summary = 30\n",
    "output_tokens = input_ids.shape[1] + tokens_for_summary\n",
    "outputs = model.generate(inputs=input_ids, do_sample=True, max_length=output_tokens)\n",
    "gen_text = tokenizer.batch_decode(outputs)[0]\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd649520-f1d4-4435-aef5-7b4026ff0ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2bced63-55d0-4e8e-81dc-c53c9f01ad3e",
   "metadata": {},
   "source": [
    "# What to do next\n",
    "- Understand the finetuning parameters\n",
    "- Is there any difference to the untrained model?\n",
    "- How well is ChatGPT doing on this task?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
