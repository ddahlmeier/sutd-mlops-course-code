{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets\n",
    "! pip install scikit-learn\n",
    "! pip install seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Rotten Tomatoes dataset\n",
    "rotten_tomatoes_dataset = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "# select random 20 samples of movie review data \n",
    "text_list = rotten_tomatoes_dataset['train'].shuffle(seed=42)[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "output_dir = \"label-output\"\n",
    "manifest_files = [f for f in listdir(output_dir) if isfile(join(output_dir, f))]\n",
    "print(manifest_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize labels to 0, 1\n",
    "map_label = {\"0\": 0,\n",
    "             \"1\": 1,\n",
    "             \"positive\": 1,\n",
    "             \"negative\": 0,\n",
    "             \"Positive\": 1,\n",
    "             \"Negative\": 0,\n",
    "             \"pOsiTive\": 1,\n",
    "             \"nEgaTiVE\": 0,\n",
    "             \"PAWsitive\": 1,\n",
    "             \"NAGitive\": 0,\n",
    "             \"pos\": 1,\n",
    "             \"neg\": 0,\n",
    "             \"2\": 1,            # one label set had label \"2\", just assuming it means positive\n",
    "            }\n",
    "\n",
    "\n",
    "def add_groundtruth_output(groundtruth_items, combined_labels):\n",
    "    for groundtruth_item in groundtruth_items.splitlines():\n",
    "        print(groundtruth_item)\n",
    "        if groundtruth_item.strip() == \"\":\n",
    "            continue\n",
    "        groundtruth_item = json.loads(groundtruth_item)\n",
    "        text = groundtruth_item['source']\n",
    "        label_job = list(filter(lambda x: x.endswith(\"metadata\"), groundtruth_item.keys()))[0]\n",
    "        label = map_label[groundtruth_item[label_job]['class-name']]\n",
    "        combined_labels[text][\"human_labels\"].append(label)\n",
    "        \n",
    "# create dictionary with review text as key and gold label as value\n",
    "combined_labels = dict(zip(text_list['text'], map(lambda x: {\"gold_label\": x, \"human_labels\": []}, text_list['label'])))\n",
    "\n",
    "for manifest_file in manifest_files:\n",
    "    with open(join(output_dir, manifest_file),'r') as f:\n",
    "        add_groundtruth_output(f.read(), combined_labels)\n",
    "    \n",
    "\n",
    "combined_labels    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "labels = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
    "\n",
    "def majority_vote(labels):\n",
    "    majority_label = max(Counter(labels).items(), key=itemgetter(1))[0]\n",
    "    return majority_label\n",
    "\n",
    "\n",
    "for text, label_dict in combined_labels.items():\n",
    "    label_dict[\"majority_label\"] = majority_vote(label_dict[\"human_labels\"])\n",
    "\n",
    "combined_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy, p/r/f1\n",
    "gold_labels = [item[\"gold_label\"]  for item in combined_labels.values()]\n",
    "majority_labels = [item[\"majority_label\"]  for item in combined_labels.values()]\n",
    "\n",
    "print(classification_report(gold_labels, majority_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cohen's kappa\n",
    "print(cohen_kappa_score(gold_labels, majority_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review counter \n",
    "review_id = 0\n",
    "\n",
    "# show review, gold labels, majority vote, and distribution plot\n",
    "review_text, label_dict = list(combined_labels.items())[review_id]\n",
    "\n",
    "print(\"Review:\", review_text)\n",
    "print(\"Gold label:\", label_dict[\"gold_label\"])\n",
    "print(\"Majority label:\", label_dict[\"majority_label\"])\n",
    "print(\"Human labels:\", label_dict[\"human_labels\"])\n",
    "print(\"Variance:\", np.var(label_dict[\"human_labels\"]))\n",
    "\n",
    "sns.countplot(x=label_dict[\"human_labels\"])\n",
    "plt.xlabel('label');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
