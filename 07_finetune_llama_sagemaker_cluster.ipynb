{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b44ac5-a6e5-4deb-bf47-9daf2db61b1a",
   "metadata": {},
   "source": [
    "# Finetune OpenLlama with Huggingface and Sagemaker cluster \n",
    "\n",
    "In this notebook, we will perform instruction tuning OpenLlama using a subset of the Dolly 15k Dataset using Sagemaker.\n",
    "\n",
    "This notebook as been put together based a few great examples and blogs. Feel free to visit them to learn more about finetuning. \n",
    "\n",
    "- [Fourthbrain Repository Building with Instruction-Tuned LLMs: a Step-by-Step Guide](https://github.com/FourthBrain/Building-with-Instruction-Tuned-LLMs-A-Step-by-Step-Guide)\n",
    "- [Fine-tune LLaMA 2 (7-70B) on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama2-qlora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12aa73-eed1-4b34-a578-5bf0fb8b4ec5",
   "metadata": {},
   "source": [
    "Please ensure your Jupyterlab instance is set to the following: **ml.t3.medium**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a983a-99df-4cbd-b8fc-cd51a4d48a93",
   "metadata": {},
   "source": [
    "# Development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985305a-85aa-48de-a454-9fe506fc8d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets[s3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7f305-bcd6-4ef9-8efc-e6b4eac00162",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5361b-9cdc-4898-923b-7d22c8f499d1",
   "metadata": {},
   "source": [
    "Let's load our dataset and get an idea of what we're working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba2329-b6ae-43d5-aa50-f4b9f7d969d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load training data set\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# Filter for question answering examples\n",
    "qa_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"closed_qa\")\n",
    "qa_dataset = qa_dataset.remove_columns(\"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_and_filter_sequence_lengths(dataset_obj, max_length=2200):\n",
    "\n",
    "    # Initialize a list to store the sequence lengths\n",
    "    sequence_lengths = []\n",
    "\n",
    "    # list of indices that are too long\n",
    "    too_long = []\n",
    "\n",
    "    # Loop over the dataset and get the lengths of text sequences\n",
    "    for idx, example in enumerate(dataset_obj):\n",
    "        sequence_lengths.append(len(example['instruction']) + len(example[\"context\"]) + len(example[\"response\"]))\n",
    "        if sequence_lengths[idx] > max_length:\n",
    "          too_long.append(idx)\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.hist(sequence_lengths, bins=30)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Text Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "    return too_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3589ae5-498f-43a6-a206-dc7b6f4ada96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter long tail of very long instances\n",
    "indexes_to_drop = plot_and_filter_sequence_lengths(qa_dataset, max_length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1445a9-8df0-4836-8f00-eab7b9d85fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset_reduced = qa_dataset.select(\n",
    "    i for i in range(len(qa_dataset)) if i not in set(indexes_to_drop)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3c45d-f5e3-4695-bd4d-4808e2fa1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = qa_dataset_reduced.train_test_split(test_size=0.1)\n",
    "train_dataset = train_and_test_dataset[\"train\"]\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_dataset.to_json(\"training.jsonl\")\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578c974-939d-4cac-8637-484ccb4dbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54dd1b-337a-4887-ad85-be2ab5c4f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"openlm-research/open_llama_3b_v2\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1cf4a-7be1-40f9-8657-14869b99df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "train_dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "\n",
    "\n",
    "# print first sample\n",
    "print(train_dataset[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f760d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(train_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f62744-73d1-4d6b-9c45-633cc5a47364",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee8944d",
   "metadata": {},
   "source": [
    "Connect to AWS S3 bucket and upload the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78417b-b196-4a93-8748-2aaeb55453e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23491bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4307c-06a1-43b1-9e71-e6c3b4517981",
   "metadata": {},
   "source": [
    "Okay, now that we have the Dolly 15k dataset pared down to a more reasonable length and uploaded to S3 - let's set up our model training job.\n",
    "\n",
    "Unlike the previous exercise, we will not run training interactively in the notebook but will submit a training run to AWS Sagemaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a3f56-ed2f-4d5b-b45c-4cf0afeb56c1",
   "metadata": {},
   "source": [
    "# Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"openlm-research/open_llama_3b_v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9ea2e-f7b9-4cf8-bee9-c72aa69645e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 1,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0af180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe60e9a",
   "metadata": {},
   "source": [
    "How much did it cost to finetune the model?\n",
    "Look up the AWS Sagemaker prices at https://aws.amazon.com/sagemaker/pricing/ and complete the cost calculation below.\n",
    "\n",
    "\n",
    "In our example for OpenLlama, the SageMaker training job took `XX seconds`. The instance we used costs `$X` per hour for on-demand usage. As a result, the total cost for finetuning was `$X`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bced63-55d0-4e8e-81dc-c53c9f01ad3e",
   "metadata": {},
   "source": [
    "# What to do next\n",
    "- deploy the mode to endpoint and do inference. Look up this blog and try to follow the instructions to deploy your model. https://www.philschmid.de/sagemaker-falcon-llm\n",
    "- compare training in the notebook with the Hugginface trainer and Jumpstart. What are pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901bea42",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
