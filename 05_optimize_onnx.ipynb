{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321fde30-f658-4963-b78c-d4ef9dc0dd19",
   "metadata": {},
   "source": [
    "# End-to-End tutorial on accelerating RoBERTa for Question-Answering including quantization and optimization\n",
    "From https://github.com/huggingface/blog/blob/main/optimum-inference.md\n",
    "\n",
    "In this End-to-End tutorial on accelerating RoBERTa for question-answering, you will learn how to:\n",
    "\n",
    "1. Install Optimum for ONNX Runtime\n",
    "2. Convert a Hugging Face Transformers model to ONNX for inference\n",
    "3. Use the ORTOptimizer to optimize the model\n",
    "4. Use the ORTQuantizer to apply dynamic quantization\n",
    "5. Run accelerated inference using Transformers pipelines\n",
    "6. Evaluate the performance and speed\n",
    "   \n",
    "Letâ€™s get started ðŸš€\n",
    "\n",
    "This tutorial was created and run on an m5.xlarge AWS EC2 Instance and also works on the SUTD cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc654fc0-a16d-4571-b3f8-b2e41e2454d6",
   "metadata": {},
   "source": [
    "## Install Optimum for Onnxruntime\n",
    "Our first step is to install Optimum with the onnxruntime utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c5f12-0313-4632-b27f-01dd8ca36466",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum[onnxruntime]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce170e-a692-45ef-b665-1543a9ef212a",
   "metadata": {},
   "source": [
    "## 3.2 Convert a Hugging Face Transformers model to ONNX for inference\n",
    "Before we can start optimizing we need to convert our vanilla transformers model to the onnx format. The model we are using is the deepset/roberta-base-squad2 a fine-tuned RoBERTa model on the SQUAD2 question answering dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64858a5-0e67-49db-9c5e-899c248e9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model_id = \"deepset/roberta-base-squad2\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "task = \"question-answering\"\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)\n",
    "\n",
    "# test the model with using transformers pipeline, with handle_impossible_answer for squad_v2\n",
    "optimum_qa = pipeline(task, model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706078b-b646-4cdf-8677-1323545a0f2c",
   "metadata": {},
   "source": [
    "We successfully converted our vanilla transformers to onnx and used the model with the transformers.pipelines to run the first prediction. Now let's optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3daaa-bb98-4917-9326-18ba2b10266d",
   "metadata": {},
   "source": [
    "## Use the ORTOptimizer to optimize the model\n",
    "After we saved our onnx checkpoint to onnx/ we can now use the ORTOptimizer to apply graph optimization, such as operator fusion and constant folding to accelerate latency and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bafe50d-a582-4e25-8ff1-b80b32796274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(onnx_path)\n",
    "\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.optimize(save_dir=onnx_path, optimization_config=optimization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb2961-1b15-4356-aa80-a0eb63c249cc",
   "metadata": {},
   "source": [
    "To test performance we can use the ORTModelForQuestionAnswering class again and provide an additional file_name parameter to load our optimized model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd1c42-7921-4b54-b037-1b9974230772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "# load optimized model\n",
    "opt_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_optimized.onnx\")\n",
    "\n",
    "# test the quantized model with using transformers pipeline\n",
    "opt_optimum_qa = pipeline(task, model=opt_model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = opt_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fc696-72b2-45af-9b2f-310ca4d70e56",
   "metadata": {},
   "source": [
    "## Use the ORTQuantizer to apply dynamic quantization\n",
    "After we have optimized our model we can accelerate it even more by quantizing it using the ORTQuantizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470da15-abf3-4b45-9eee-65a8bc42de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(onnx_path, file_name=\"model_optimized.onnx\")\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "quantizer.quantize(save_dir=onnx_path, quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784792ee-46d7-47f6-b995-3ce99fdb9d35",
   "metadata": {},
   "source": [
    "We can now compare this model size as well as some latency performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af92e2c-a508-4f56-9f2a-adb2a5419706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "print(f\"Vanilla Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(onnx_path / \"model_optimized_quantized.onnx\")/(1024*1024)\n",
    "print(f\"Quantized Onnx Model file size: {size:.2f} MB\")\n",
    "\n",
    "# Vanilla Onnx Model file size: 473.51 MB\n",
    "# Quantized Onnx Model file size: 119.15 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c57cd0-2c1e-4972-8348-1a50cde5dea2",
   "metadata": {},
   "source": [
    "## Run accelerated inference using pipelines\n",
    "\n",
    "Optimum has built-in support for transformers pipelines. This allows us to leverage the same API that we know from using PyTorch and TensorFlow models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cde70-69eb-418a-b3e7-e79d298c618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_optimized_quantized.onnx\")\n",
    "\n",
    "quantized_optimum_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "prediction = quantized_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ab4bb-bb57-490e-862a-727d9fbe4b4c",
   "metadata": {},
   "source": [
    "In addition to this optimum has a pipelines API which guarantees more safety for your accelerated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cf453-8153-44ad-9fb2-3790482f6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "from optimum.pipelines import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_path, file_name=\"model_optimized_quantized.onnx\")\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_optimized_quantized.onnx\")\n",
    "                                                     \n",
    "quantized_optimum_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = quantized_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb03c5-74cd-42f7-bcfe-02b53c5a095d",
   "metadata": {},
   "source": [
    "## Evaluate performance and speed\n",
    "As the last step, we want to take a detailed look at the performance and accuracy of our model. Applying optimization techniques, like graph optimizations or quantization not only impact performance (latency) those also might have an impact on the accuracy of the model. So accelerating your model comes with a trade-off.\n",
    "\n",
    "Let's evaluate our models. Our transformers model deepset/roberta-base-squad2 was fine-tuned on the SQUAD2 dataset. This will be the dataset we use to evaluate our models.\n",
    "To safe time, we only load 10% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508dc2c-9d64-4193-89c8-59aff4381329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric,load_dataset\n",
    "\n",
    "# load 10% of the data to safe time\n",
    "metric = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"squad_v2\", split=\"validation[:10%]\")\n",
    "\n",
    "print(f\"length of dataset {len(dataset)}\")\n",
    "#length of dataset 1187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df13f1-b2ef-4db8-86cc-fe2fcc93c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(example):\n",
    "  default = optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  optimized = opt_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  quantized = quantized_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  return {\n",
    "      'reference': {'id': example['id'], 'answers': example['answers']},\n",
    "      'default': {'id': example['id'],'prediction_text': default['answer'], 'no_answer_probability': 0.},\n",
    "      'optimized': {'id': example['id'],'prediction_text': optimized['answer'], 'no_answer_probability': 0.},\n",
    "      'quantized': {'id': example['id'],'prediction_text': quantized['answer'], 'no_answer_probability': 0.},\n",
    "      }\n",
    "\n",
    "result = dataset.map(evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea9364-d017-4cde-a394-6d6846c8e32e",
   "metadata": {},
   "source": [
    "Now lets compare the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22246ce-d719-4323-b5c8-5678064a1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_acc = metric.compute(predictions=result[\"default\"], references=result[\"reference\"])\n",
    "optimized = metric.compute(predictions=result[\"optimized\"], references=result[\"reference\"])\n",
    "quantized = metric.compute(predictions=result[\"quantized\"], references=result[\"reference\"])\n",
    "\n",
    "print(f\"vanilla model: exact={default_acc['exact']}% f1={default_acc['f1']}%\")\n",
    "print(f\"optimized model: exact={optimized['exact']}% f1={optimized['f1']}%\")\n",
    "print(f\"quantized model: exact={quantized['exact']}% f1={quantized['f1']}%\")\n",
    "\n",
    "# vanilla model: exact=81.12889637742207% f1=83.27089343306695%\n",
    "# optimized model: exact=81.12889637742207% f1=83.27089343306695%\n",
    "# quantized model: exact=79.78096040438079% f1=82.22196132052908%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a2b7b-cc99-46b1-8c2a-56a804b2c37f",
   "metadata": {},
   "source": [
    "Our optimized & quantized model achieved an exact match of 79.78% and an f1 score of 82.22% which is 98.74% of the original accuracy.\n",
    "\n",
    "Okay, let's test the performance (latency) of our optimized and quantized model.\n",
    "\n",
    "But first, letâ€™s extend our context and question to a more meaningful sequence length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f32a3-65c5-4e0b-b72f-fc1b52a8948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"Hello, my name is Philipp and I live in Nuremberg, Germany. Currently I am working as a Technical Lead at Hugging Face to democratize artificial intelligence through open source and open science. In the past I designed and implemented cloud-native machine learning architectures for fin-tech and insurance companies. I found my passion for cloud concepts and machine learning 5 years ago. Since then I never stopped learning. Currently, I am focusing myself in the area NLP and how to leverage models like BERT, Roberta, T5, ViT, and GPT2 to generate business value.\"\n",
    "question=\"As what is Philipp working?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec406102-ba65-4d05-bb10-10703753be58",
   "metadata": {},
   "source": [
    "To keep it simple, we are going to use a python loop and calculate the avg/mean latency for our vanilla model and for the optimized and quantized model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef28ced-87a0-4096-9a43-2dcdf3f6a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(question=question, context=context)\n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(question=question, context=context)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    return f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\"\n",
    "\n",
    "print(f\"Vanilla model {measure_latency(optimum_qa)}\")\n",
    "print(f\"Optimized & Quantized model {measure_latency(quantized_optimum_qa)}\")\n",
    "\n",
    "# Vanilla model Average latency (ms) 126\n",
    "# Optimized & Quantized model Average latency (ms) 89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245313e-f7ea-427b-8fd5-d7ea3efdab68",
   "metadata": {},
   "source": [
    "We managed to reduce our model latency from 126ms to 89ms or by roughly 30%, while keeping 98% of the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349b494-aebf-4230-8f05-c09a14105cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
