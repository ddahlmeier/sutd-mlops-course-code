{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321fde30-f658-4963-b78c-d4ef9dc0dd19",
   "metadata": {},
   "source": [
    "# End-to-End tutorial on accelerating RoBERTa for Question-Answering including quantization and optimization\n",
    "From https://github.com/huggingface/blog/blob/main/optimum-inference.md\n",
    "\n",
    "In this End-to-End tutorial on accelerating RoBERTa for question-answering, you will learn how to:\n",
    "\n",
    "1. Install Optimum for ONNX Runtime\n",
    "2. Convert a Hugging Face Transformers model to ONNX for inference\n",
    "3. Use the ORTOptimizer to optimize the model\n",
    "4. Use the ORTQuantizer to apply dynamic quantization\n",
    "5. Run accelerated inference using Transformers pipelines\n",
    "6. Evaluate the performance and speed\n",
    "   \n",
    "Letâ€™s get started ðŸš€\n",
    "\n",
    "This tutorial was created and run on an m5.xlarge AWS EC2 Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc654fc0-a16d-4571-b3f8-b2e41e2454d6",
   "metadata": {},
   "source": [
    "## Install Optimum for Onnxruntime\n",
    "Our first step is to install Optimum with the onnxruntime utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0c5f12-0313-4632-b27f-01dd8ca36466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum[onnxruntime] in /opt/conda/lib/python3.11/site-packages (1.16.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (15.0.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (1.12)\n",
      "Requirement already satisfied: transformers>=4.26.0 in /opt/conda/lib/python3.11/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (4.36.2)\n",
      "Requirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (2.1.1+cu118)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (23.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (1.26.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (0.20.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (2.16.1)\n",
      "Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (1.15.0)\n",
      "Requirement already satisfied: onnxruntime>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (1.16.3)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (0.4.1)\n",
      "Requirement already satisfied: protobuf>=3.20.1 in /opt/conda/lib/python3.11/site-packages (from optimum[onnxruntime]) (4.25.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->optimum[onnxruntime]) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (3.8.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets->optimum[onnxruntime]) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (4.8.0)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.11/site-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (23.5.26)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.9->optimum[onnxruntime]) (3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.9->optimum[onnxruntime]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.9->optimum[onnxruntime]) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (0.4.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.11/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.11/site-packages (from coloredlogs->optimum[onnxruntime]) (10.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.11/site-packages (from evaluate->optimum[onnxruntime]) (0.18.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->optimum[onnxruntime]) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets->optimum[onnxruntime]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets->optimum[onnxruntime]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets->optimum[onnxruntime]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.9->optimum[onnxruntime]) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->optimum[onnxruntime]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->optimum[onnxruntime]) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->optimum[onnxruntime]) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum[onnxruntime]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum[onnxruntime]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce170e-a692-45ef-b665-1543a9ef212a",
   "metadata": {},
   "source": [
    "## 3.2 Convert a Hugging Face Transformers model to ONNX for inference\n",
    "Before we can start optimizing we need to convert our vanilla transformers model to the onnx format. The model we are using is the deepset/roberta-base-squad2 a fine-tuned RoBERTa model on the SQUAD2 question answering dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64858a5-0e67-49db-9c5e-899c248e9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `from_transformers` is deprecated, and will be removed in optimum 2.0.  Use `export` instead\n",
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.1+cu118\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9041661620140076, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model_id = \"deepset/roberta-base-squad2\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "task = \"question-answering\"\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)\n",
    "\n",
    "# test the model with using transformers pipeline, with handle_impossible_answer for squad_v2\n",
    "optimum_qa = pipeline(task, model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706078b-b646-4cdf-8677-1323545a0f2c",
   "metadata": {},
   "source": [
    "We successfully converted our vanilla transformers to onnx and used the model with the transformers.pipelines to run the first prediction. Now let's optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3daaa-bb98-4917-9326-18ba2b10266d",
   "metadata": {},
   "source": [
    "## Use the ORTOptimizer to optimize the model\n",
    "After we saved our onnx checkpoint to onnx/ we can now use the ORTOptimizer to apply graph optimization such as operator fusion and constant folding to accelerate latency and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bafe50d-a582-4e25-8ff1-b80b32796274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/optimum/onnxruntime/configuration.py:770: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "Optimizing model...\n",
      "2024-01-06 15:23:26.790059717 [W:onnxruntime:, inference_session.cc:1732 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n",
      "Configuration saved in onnx/ort_config.json\n",
      "Optimized model saved at: onnx (external data format: False; saved all tensor to one file: True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "#optimizer = ORTOptimizer.from_pretrained(model_id, feature=task)\n",
    "optimizer = ORTOptimizer.from_pretrained(onnx_path)\n",
    "\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.optimize(save_dir=onnx_path, optimization_config=optimization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb2961-1b15-4356-aa80-a0eb63c249cc",
   "metadata": {},
   "source": [
    "To test performance we can use the ORTModelForQuestionAnswering class again and provide an additional file_name parameter to load our optimized model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fd1c42-7921-4b54-b037-1b9974230772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9041661620140076, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "# load optimized model\n",
    "opt_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_optimized.onnx\")\n",
    "\n",
    "# test the quantized model with using transformers pipeline\n",
    "opt_optimum_qa = pipeline(task, model=opt_model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = opt_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fc696-72b2-45af-9b2f-310ca4d70e56",
   "metadata": {},
   "source": [
    "## Use the ORTQuantizer to apply dynamic quantization\n",
    "After we have optimized our model we can accelerate it even more by quantizing it using the ORTQuantizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470da15-abf3-4b45-9eee-65a8bc42de76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: True)\n",
      "Quantizing model...\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(onnx_path, file_name=\"model_optimized.onnx\")\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "quantizer.quantize(save_dir=onnx_path, quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784792ee-46d7-47f6-b995-3ce99fdb9d35",
   "metadata": {},
   "source": [
    "We can now compare this model size as well as some latency performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af92e2c-a508-4f56-9f2a-adb2a5419706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "print(f\"Vanilla Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(onnx_path / \"model_optimized_quantized.onnx\")/(1024*1024)\n",
    "print(f\"Quantized Onnx Model file size: {size:.2f} MB\")\n",
    "\n",
    "# Vanilla Onnx Model file size: 473.51 MB\n",
    "# Quantized Onnx Model file size: 119.15 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c57cd0-2c1e-4972-8348-1a50cde5dea2",
   "metadata": {},
   "source": [
    "## Run accelerated inference using pipelines\n",
    "\n",
    "Optimum has built-in support for transformers pipelines. This allows us to leverage the same API that we know from using PyTorch and TensorFlow models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cde70-69eb-418a-b3e7-e79d298c618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_optimized_quantized.onnx\")\n",
    "\n",
    "quantized_optimum_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "prediction = quantized_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ab4bb-bb57-490e-862a-727d9fbe4b4c",
   "metadata": {},
   "source": [
    "In addition to this optimum has a pipelines API which guarantees more safety for your accelerated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cf453-8153-44ad-9fb2-3790482f6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "from optimum.pipelines import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_path, file_name=\"model_optimized_quantized.onnx\")\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_optimized_quantized.onnx\")\n",
    "                                                     \n",
    "quantized_optimum_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = quantized_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb03c5-74cd-42f7-bcfe-02b53c5a095d",
   "metadata": {},
   "source": [
    "## Evaluate performance and speed\n",
    "As the last step, we want to take a detailed look at the performance and accuracy of our model. Applying optimization techniques, like graph optimizations or quantization not only impact performance (latency) those also might have an impact on the accuracy of the model. So accelerating your model comes with a trade-off.\n",
    "\n",
    "Let's evaluate our models. Our transformers model deepset/roberta-base-squad2 was fine-tuned on the SQUAD2 dataset. This will be the dataset we use to evaluate our models.\n",
    "To safe time, we only load 10% of the dataset but this step still takes about 30min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508dc2c-9d64-4193-89c8-59aff4381329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric,load_dataset\n",
    "\n",
    "# load 10% of the data to safe time\n",
    "metric = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"squad_v2\", split=\"validation[:10%]\")\n",
    "\n",
    "print(f\"length of dataset {len(dataset)}\")\n",
    "#length of dataset 1187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df13f1-b2ef-4db8-86cc-fe2fcc93c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(example):\n",
    "  default = optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  optimized = opt_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  quantized = quantized_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  return {\n",
    "      'reference': {'id': example['id'], 'answers': example['answers']},\n",
    "      'default': {'id': example['id'],'prediction_text': default['answer'], 'no_answer_probability': 0.},\n",
    "      'optimized': {'id': example['id'],'prediction_text': optimized['answer'], 'no_answer_probability': 0.},\n",
    "      'quantized': {'id': example['id'],'prediction_text': quantized['answer'], 'no_answer_probability': 0.},\n",
    "      }\n",
    "\n",
    "result = dataset.map(evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea9364-d017-4cde-a394-6d6846c8e32e",
   "metadata": {},
   "source": [
    "Now lets compare the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22246ce-d719-4323-b5c8-5678064a1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_acc = metric.compute(predictions=result[\"default\"], references=result[\"reference\"])\n",
    "optimized = metric.compute(predictions=result[\"optimized\"], references=result[\"reference\"])\n",
    "quantized = metric.compute(predictions=result[\"quantized\"], references=result[\"reference\"])\n",
    "\n",
    "print(f\"vanilla model: exact={default_acc['exact']}% f1={default_acc['f1']}%\")\n",
    "print(f\"optimized model: exact={optimized['exact']}% f1={optimized['f1']}%\")\n",
    "print(f\"quantized model: exact={quantized['exact']}% f1={quantized['f1']}%\")\n",
    "\n",
    "# vanilla model: exact=79.07858165585783% f1=82.14970024570314%\n",
    "# optimized model: exact=79.07858165585783% f1=82.14970024570314%\n",
    "# quantized model: exact=78.75010528088941% f1=81.82526107204629%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a2b7b-cc99-46b1-8c2a-56a804b2c37f",
   "metadata": {},
   "source": [
    "Our optimized & quantized model achieved an exact match of 78.75% and an f1 score of 81.83% which is 99.61% of the original accuracy. Achieving 99% of the original model is very good especially since we used dynamic quantization.\n",
    "\n",
    "Okay, let's test the performance (latency) of our optimized and quantized model.\n",
    "\n",
    "But first, letâ€™s extend our context and question to a more meaningful sequence length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f32a3-65c5-4e0b-b72f-fc1b52a8948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"Hello, my name is Philipp and I live in Nuremberg, Germany. Currently I am working as a Technical Lead at Hugging Face to democratize artificial intelligence through open source and open science. In the past I designed and implemented cloud-native machine learning architectures for fin-tech and insurance companies. I found my passion for cloud concepts and machine learning 5 years ago. Since then I never stopped learning. Currently, I am focusing myself in the area NLP and how to leverage models like BERT, Roberta, T5, ViT, and GPT2 to generate business value.\"\n",
    "question=\"As what is Philipp working?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec406102-ba65-4d05-bb10-10703753be58",
   "metadata": {},
   "source": [
    "To keep it simple, we are going to use a python loop and calculate the avg/mean latency for our vanilla model and for the optimized and quantized model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef28ced-87a0-4096-9a43-2dcdf3f6a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(question=question, context=context)\n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(question=question, context=context)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    return f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\"\n",
    "\n",
    "print(f\"Vanilla model {measure_latency(optimum_qa)}\")\n",
    "print(f\"Optimized & Quantized model {measure_latency(quantized_optimum_qa)}\")\n",
    "\n",
    "# Vanilla model Average latency (ms) - 117.61 +\\- 8.48\n",
    "# Optimized & Quantized model Average latency (ms) - 64.94 +\\- 3.65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245313e-f7ea-427b-8fd5-d7ea3efdab68",
   "metadata": {},
   "source": [
    "We managed to accelerate our model latency from 117.61ms to 64.94ms or roughly 2x while keeping 99.61% of the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349b494-aebf-4230-8f05-c09a14105cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
