{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G4M3L6i7Y9v"
   },
   "source": [
    "# Using LLM-as-a-judge ðŸ§‘â€âš–ï¸ for an automated and versatile evaluation\n",
    "_From : [Huggingface](https://huggingface.co/learn/cookbook/llm_judge) authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_ \n",
    "\n",
    "Evaluation of Large language models (LLMs) is often a difficult endeavour: given their broad capabilities, the tasks given to them often should be judged on requirements that would be very broad, and loosely-defined. For instance, an assistant's answer to a question can be:\n",
    "- not grounded in context\n",
    "- repetitive, repetitive, repetitive\n",
    "- grammatically incorrects\n",
    "- Excessively lengthy and characterized by an overabundance of words, leading to a situation where the discourse or written content becomes overly detailed and protracted\n",
    "- incoherent\n",
    "- ...\n",
    "\n",
    "The list of criteria goes on and on. And even if we had a limited list, each of these would be hard to measure: \"devising a rule-based program to assess the outputs is extremely challenging. Traditional evaluation metrics based on the similarity between outputs and reference answers (e.g., ROUGE, BLEU) are also ineffective for these questions.\"\n",
    "\n",
    "âœ… A powerful solution to assess outputs in a human way, without requiring costly human time, is LLM-as-a-judge.\n",
    "This method was introduced in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://huggingface.co/papers/2306.05685) - which I encourage you to read.\n",
    "\n",
    "ðŸ’¡ The idea is simple: ask an LLM to do the grading for you. ðŸ¤–âœ“\n",
    "\n",
    "But we'll see that it will not work well out-of-the-box: you need to set it up carefully for good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AaO06QoA7Y9x",
    "outputId": "a92fb755-2215-4ccf-b98f-0b6aee8f5bf0"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub datasets pandas tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Nzc9OQM7Y9y"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, DownloadMode\n",
    "from huggingface_hub import InferenceClient, notebook_login, login\n",
    "\n",
    "tqdm.pandas()  # load tqdm's pandas support\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "notebook_login() # need to login to huggingface with access token  https://huggingface.co/settings/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CDizybf7Y9y",
    "outputId": "a6ec4d4e-9ef5-4960-c31c-2e67bef02861"
   },
   "outputs": [],
   "source": [
    "# This is a gated model.\n",
    "# You need to login in to huggingface and get access to the model\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "# Test your LLM client\n",
    "llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JculZ__u7Y9z"
   },
   "source": [
    "## 1. Prepare the creation and evaluation of our LLM judge\n",
    "\n",
    "Let's say you want to give an LLM a specific task, like answering open-ended questions.\n",
    "\n",
    "The difficulty is that, as we discussed above, measuring the answer's quality is difficult, for instance an exact string match will flag too many correct but differently worded answers as false.\n",
    "\n",
    "You could get human labellers to judge the outputs, but this is very time-consuming for them, and if you want to update the model or the questions, you have to do it all over again.\n",
    "\n",
    "âœ… In this case you can setup a LLM-as-a-judge.\n",
    "\n",
    "**But to use a LLM-as-a-judge, you will first need to evaluate how reliably it rates your model outputs.**\n",
    "\n",
    "âž¡ï¸ So the first step will be... To create a human evaluation dataset. But you can get human annotations for a few examples only - something like 30 should be enough to get a good idea of the performance.\n",
    "And you will be able to re-use this dataset everytime you want to test your LLM-as-a-judge.\n",
    "\n",
    "In our case, we will use [`feedbackQA`](https://huggingface.co/datasets/McGill-NLP/feedbackQA), which contains 2 human evaluations and scores for each question/answer couple: using a sample of 30 examples will be representative of what your small evaluation dataset could be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data with huggingface dataset\n",
    "# # This sometimes fails, alternative approach is below\n",
    "\n",
    "# ratings = load_dataset(\"McGill-NLP/feedbackQA\")[\"train\"]\n",
    "# ratings = pd.DataFrame(ratings)\n",
    "\n",
    "# ratings[\"review_1\"] = ratings[\"feedback\"].apply(lambda x: x[\"rating\"][0])\n",
    "# ratings[\"explanation_1\"] = ratings[\"feedback\"].apply(lambda x: x[\"explanation\"][0])\n",
    "# ratings[\"review_2\"] = ratings[\"feedback\"].apply(lambda x: x[\"rating\"][1])\n",
    "# ratings[\"explanation_2\"] = ratings[\"feedback\"].apply(lambda x: x[\"explanation\"][1])\n",
    "# ratings = ratings.drop(columns=[\"feedback\"])\n",
    "\n",
    "# # Map scores to numeric values\n",
    "# conversion_dict = {\"Excellent\": 4, \"Acceptable\": 3, \"Could be Improved\": 2, \"Bad\": 1}\n",
    "# ratings[\"score_1\"] = ratings[\"review_1\"].map(conversion_dict)\n",
    "# ratings[\"score_2\"] = ratings[\"review_2\"].map(conversion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative data download from github directly\n",
    "! git clone https://github.com/McGill-NLP/feedbackqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from clonced raw json files\n",
    "import json\n",
    "\n",
    "def generate_examples(data_json):\n",
    "    key = 0\n",
    "    for dict_item in data_json:\n",
    "        question = dict_item['question']\n",
    "        passage_text = ''\n",
    "        if dict_item['passage']['reference']['page_title']:\n",
    "            passage_text += dict_item['passage']['reference']['page_title'] + '\\n'\n",
    "        if dict_item['passage']['reference']['section_headers']:\n",
    "            passage_text += '\\n'.join(dict_item['passage']['reference']['section_headers']) + '\\n'\n",
    "        if dict_item['passage']['reference']['section_content']:\n",
    "            passage_text += dict_item['passage']['reference']['section_content']\n",
    "\n",
    "        yield {\"id\": key,\n",
    "               \"question\": question,\n",
    "               \"answer\": passage_text,\n",
    "               \"review_1\": dict_item['rating'][0],\n",
    "               \"review_2\": dict_item['rating'][1],\n",
    "               \"explanation_1\": dict_item['feedback'][0],\n",
    "               \"explanation_2\": dict_item['feedback'][1]\n",
    "              }\n",
    "        key += 1\n",
    "        \n",
    "# Let's load the training set \n",
    "data_qa = json.load(open('./feedbackqa/data/feedback_train.json'))\n",
    "ratings = pd.DataFrame(generate_examples(data_qa))\n",
    "\n",
    "# Map scores to numeric values\n",
    "conversion_dict = {\"Excellent\": 4, \"Acceptable\": 3, \"Could be Improved\": 2, \"Bad\": 1}\n",
    "ratings[\"score_1\"] = ratings[\"review_1\"].map(conversion_dict)\n",
    "ratings[\"score_2\"] = ratings[\"review_2\"].map(conversion_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the resulting dataframe\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfiQ9uoI7Y9z"
   },
   "source": [
    "It's always a good idea to compute a baseline for performance: here it can be for instance the agreement between the two human raters, as measured by the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) of the scores they give."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9onEmIt7Y9z",
    "outputId": "cd8721c6-2b55-4051-ad1d-17c6e69ebd7e"
   },
   "outputs": [],
   "source": [
    "print(\"Correlation between 2 human raters:\")\n",
    "print(f\"{ratings['score_1'].corr(ratings['score_2'], method='pearson'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ah9iBGzo7Y90"
   },
   "source": [
    "This correlation between 2 human raters is not that good. If your human ratings are really bad, it probably means the rating criteria are not clear enough.\n",
    "\n",
    "This means that our \"ground truth\" contains noise: hence we cannot expect any algorithmic evaluation to come that close to it.\n",
    "\n",
    "However, we could reduce this noise:\n",
    "- by taking the average score as our ground truth instead of any single score, we should even out some of the irregularities.\n",
    "- by only selecting the samples where the human reviewers are in agreement.\n",
    "\n",
    "Here, we will choose the last option and **only keep examples where the 2 human reviewers are in agreement**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgwRk_2i7Y90",
    "outputId": "32753c49-a112-4907-f427-4404b2f401f6"
   },
   "outputs": [],
   "source": [
    "# Sample examples\n",
    "ratings_where_raters_agree = ratings.loc[ratings[\"score_1\"] == ratings[\"score_2\"]]\n",
    "examples = ratings_where_raters_agree.groupby(\"score_1\").sample(7, random_state=1214)\n",
    "examples[\"human_score\"] = examples[\"score_1\"]\n",
    "\n",
    "# Visualize 1 sample for each score\n",
    "display(examples.groupby(\"human_score\").first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeBfXFxN7Y90"
   },
   "source": [
    "## 2. Create our LLM judge\n",
    "We build our LLM judge with a basic prompt, containing these elements:\n",
    "- task description\n",
    "- scale description: `minimum`, `maximum`, value types (`float` here)\n",
    "- explanation of the output format\n",
    "- a beginning of an answer, to take the LLM by the hand as far as we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSdZIKhr7Y90"
   },
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"\n",
    "You will be given a user_question and system_answer couple.\n",
    "Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n",
    "Give your answer as a float on a scale of 0 to 10, where 0 means that the system_answer is not helpful at all, and 10 means that the answer completely and helpfully addresses the question.\n",
    "\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Total rating: (your rating, as a float between 0 and 10)\n",
    "\n",
    "Now here are the question and answer.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Feedback:::\n",
    "Total rating: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpuMfOko7Y90"
   },
   "outputs": [],
   "source": [
    "examples[\"llm_judge\"] = examples.progress_apply(\n",
    "    lambda x: llm_client.text_generation(\n",
    "        prompt=JUDGE_PROMPT.format(question=x[\"question\"], answer=x[\"answer\"]),\n",
    "        max_new_tokens=1000,\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGEqUKpX7Y90"
   },
   "outputs": [],
   "source": [
    "def extract_judge_score(answer: str, split_str: str = \"Total rating:\") -> int:\n",
    "    try:\n",
    "        if split_str in answer:\n",
    "            rating = answer.split(split_str)[1]\n",
    "        else:\n",
    "            rating = answer\n",
    "        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n",
    "        return float(digit_groups[0])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "examples[\"llm_judge_score\"] = examples[\"llm_judge\"].apply(extract_judge_score)\n",
    "# Rescale the score given by the LLM on the same scale as the human score\n",
    "examples[\"llm_judge_score\"] = (examples[\"llm_judge_score\"] / 10) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DXs3YOF7Y90",
    "outputId": "20a95785-6398-48fe-857b-da17bd6b5108"
   },
   "outputs": [],
   "source": [
    "print(\"Correlation between LLM-as-a-judge and the human raters:\")\n",
    "print(\n",
    "    f\"{examples['llm_judge_score'].corr(examples['human_score'], method='pearson'):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNSuWrpK7Y91"
   },
   "source": [
    "This is not bad, given that the Pearson correlation between 2 random, independent variables would be 0!\n",
    "\n",
    "But we easily can do better. ðŸ”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vew-oU5V7Y91"
   },
   "source": [
    "## 3. Improve the LLM judge\n",
    "\n",
    "As shown by [Aparna Dhinakaran](https://twitter.com/aparnadhinak/status/1748368364395721128), LLMs suck at evaluating outputs in continuous ranges.\n",
    "[This article](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG) gives us a few best practices to build a better prompt:\n",
    "- â³ **Leave more time for thought** by adding an `Evaluation` field before the final answer.\n",
    "- ðŸ”¢ **Use a small integer scale** like 1-4 or 1-5 instead of a large float scale as we had previously.\n",
    "- ðŸ‘©â€ðŸ« **Provide an indicative scale for guidance**.\n",
    "- We even add a carrot to motivate the LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6o0gV-c7Y91"
   },
   "outputs": [],
   "source": [
    "IMPROVED_JUDGE_PROMPT = \"\"\"\n",
    "You will be given a user_question and system_answer couple.\n",
    "Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n",
    "Give your answer on a scale of 1 to 4, where 1 means that the system_answer is not helpful at all, and 4 means that the system_answer completely and helpfully addresses the user_question.\n",
    "\n",
    "Here is the scale you should use to build your answer:\n",
    "1: The system_answer is terrible: completely irrelevant to the question asked, or very partial\n",
    "2: The system_answer is mostly not helpful: misses some key aspects of the question\n",
    "3: The system_answer is mostly helpful: provides support, but still could be improved\n",
    "4: The system_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the question\n",
    "\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 4)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and answer.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.\n",
    "Feedback:::\n",
    "Evaluation: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ocy5Gijy7Y91"
   },
   "outputs": [],
   "source": [
    "examples[\"llm_judge_improved\"] = examples.progress_apply(\n",
    "    lambda x: llm_client.text_generation(\n",
    "        prompt=IMPROVED_JUDGE_PROMPT.format(question=x[\"question\"], answer=x[\"answer\"]),\n",
    "        max_new_tokens=500,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "examples[\"llm_judge_improved_score\"] = examples[\"llm_judge_improved\"].apply(\n",
    "    extract_judge_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQVBNVZB7Y91",
    "outputId": "5ef321a6-4253-4fa1-efdc-eca14ed1d467"
   },
   "outputs": [],
   "source": [
    "print(\"Correlation between LLM-as-a-judge and the human raters:\")\n",
    "print(\n",
    "    f\"{examples['llm_judge_improved_score'].corr(examples['human_score'], method='pearson'):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3wKCCK67Y91"
   },
   "source": [
    "The correlation was **improved by nearly 30%** with only a few tweaks to the prompt (of which  a few percentage points are due to my shameless tip to the LLM, which I hereby declare not legally binding).\n",
    "\n",
    "Quite impressive! ðŸ‘\n",
    "\n",
    "Let's display a few errors of our LLM judge to analyse them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zPrRvGN7Y91",
    "outputId": "29d35171-8855-41d0-afd3-63c45ac5c35e"
   },
   "outputs": [],
   "source": [
    "errors = pd.concat(\n",
    "    [\n",
    "        examples.loc[\n",
    "            examples[\"llm_judge_improved_score\"] > examples[\"human_score\"]\n",
    "        ].head(1),\n",
    "        examples.loc[\n",
    "            examples[\"llm_judge_improved_score\"] < examples[\"human_score\"]\n",
    "        ].head(2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(\n",
    "    errors[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"human_score\",\n",
    "            \"explanation_1\",\n",
    "            \"llm_judge_improved_score\",\n",
    "            \"llm_judge_improved\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czohC4td7Y91"
   },
   "source": [
    "The disagreements are minor: overall, we seem to have reached a good level of performance for our system!\n",
    "\n",
    "## 4. How do we take our LLM judge even further?\n",
    "\n",
    "ðŸŽ¯ **You will never reach 100%:** Let's first note that our human ground truth certainly has some noise, so agreement/correlation will never go up to 100% even with a perfect LLM judge.\n",
    "\n",
    "ðŸ§­ **Provide a reference:** If you had access to a reference answer for each question, you should definitely give this to the Judge LLM in its prompt to get better results!\n",
    "\n",
    "â–¶ï¸ **Provide few-shot examples:** adding some few-shot examples of questions and ground truth evaluations in the prompt can improve the results. _(I tried it here, it did not improve results in this case so I skipped it, but it could work for your dataset!)_\n",
    "\n",
    "âž• **Additive scale:** When the judgement can be split into atomic criteria, using an additive scale can further improve results: see below ðŸ‘‡\n",
    "```python\n",
    "ADDITIVE_PROMPT = \"\"\"\n",
    "(...)\n",
    "- Award 1 point if the answer is related to the question.\n",
    "- Give 1 additional point if the answer is clear and precise.\n",
    "- Provide 1 further point if the answer is true.\n",
    "- One final point should be awarded if the answer provides additional resources to support the user.\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Implement with structured generation:**\n",
    "\n",
    "Using **structured generation**, you can configure the LLM judge to directly provide its output as a JSON with fields `Evaluation` and `Total rating`, which makes parsing easier : see our [structured generation](structured_generation) cookbook to learn more!\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "That's all for today, congrats for following along! ðŸ¥³\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
